"use strict";(self.webpackChunklayotto_docusaurus=self.webpackChunklayotto_docusaurus||[]).push([[8123],{609:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/exploration-and-practice-of-antcloud-native-application-runtime-archsummit-shanghai","metadata":{"permalink":"/en-US/blog/exploration-and-practice-of-antcloud-native-application-runtime-archsummit-shanghai","editUrl":"https://github.com/mosn/layotto/edit/main//i18n/en-US/docusaurus-plugin-content-blog/exploration-and-practice-of-antcloud-native-application-runtime-archsummit-shanghai.md","source":"@site/i18n/en-US/docusaurus-plugin-content-blog/exploration-and-practice-of-antcloud-native-application-runtime-archsummit-shanghai.md","title":"Ant Cloud Native Apps Exploring and Practice - ArchiSummit","description":"The introduction of the Mesh model is a key path to the application of clouds and ant groups have achieved mass landings internally.The sinking of more middleware capabilities, such as Message, DB, Cache Mesh and others, will be the future shape of intermediate technology when the app evolves from Mesh.Apps run to help developers construct cloud native apps quickly and to further decouple apps and infrastructure, while the app runs at the core of API standards, the community is expected to build together.","date":"2024-07-05T07:00:37.000Z","tags":[],"readingTime":13.02,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"Source Parse 4 Layer Traffic Governance, tcp traffic dump","permalink":"/en-US/blog/tcpcopy_code_analyze"}},"content":"> The introduction of the Mesh model is a key path to the application of clouds and ant groups have achieved mass landings internally.The sinking of more middleware capabilities, such as Message, DB, Cache Mesh and others, will be the future shape of intermediate technology when the app evolves from Mesh.Apps run to help developers construct cloud native apps quickly and to further decouple apps and infrastructure, while the app runs at the core of API standards, the community is expected to build together.\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*nergRo8-RI0AAAAAAAAAAAAAARQnAQ)\\n\\n## Ant Group Mesh Introduction\\n\\nAnt is a technology and innovation-driven company, from its earliest days as a payment app on Taobao to its current services\\nAs a large company with 1.2 billion users worldwide, Ant\'s technical architecture evolution will probably be divided into the following stages:\\n\\nPrior to 2006, the earliest payment was a centralized monolithic application with modular development of different businesses.\\n\\nIn 2007, as more scenes of payments were promoted, an application and data splitting began to be made and some modifications to SOA were made.\\n\\nAfter 2010, rapid payments, mobile payments, support for two-eleven and balance jewels have been introduced, and users have reached the level of hundreds of millions, and the number of ant applications has grown, and ants have developed many full sets of microservice middleware to support ant operations;\\n\\nIn 2014, like the advent of more business formalities like rush flow, online payments and more scenes, higher requirements for ant availability and stability, ants supported LDC moderation in microservice intermediation, off-site support for business support, and elasticity scaling-up in mixed clouds that support bi-11 ultra-mass traffic.\\n\\nIn 2020, ant business was not only digital finance, but also the emergence of new strategies such as digital life and internationalization, which prompted us to have a more efficient technical structure that would allow the operation to run faster and more steadily, so ant ants were able to internalize a more popular concept of cloud-origin in the industry.\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*KCSVTZWSf8wAAAAAAAAAAAAAARQnAQ)\\n\\nThe technical structure of ant can also be seen to evolve along with the business innovations of the company from centralization to SOA to microservices, believing that the classmates with microservices are well known and that the practice of microservices to clouds has been explored by ants themselves in recent years.\\n\\n## Why to introduce Service Mesh\\n\\nSince ant has a complete set of microservice governance intermediaries, why do you need to introduce Service Mesh?\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*Sq7oR6eO2QAAAAAAAAAAAAAAARQnAQ)\\n\\nThe service framework for ant self-research is SOFARPC as an example of a powerful SDK that includes a range of capabilities such as discovery of services, routing, melting out streams, etc.In a basic SOFA(Javaa) app, business code integrates SOFARP\'s SDK, both running in a process.After the large scale of sunk microservice, we faced some of the following problems with\uff1a\\n\\n**Upgrade cost**\uff1aSDK requires business code introduction. Each upgrade requires a change code to be published.Because of the large scale of applications, some major technological changes or safety problems are being repaired.It takes thousands of apps to upgrade each time it takes time.\\n**Version Fragment**\uff1ais highly fragmented, due to the high cost of upgrades, which makes it difficult for us to use historical logic when writing our code and to evolve across technology.\\n**Cross-language is unmanageable**\uff1aant online applications mostly use Java as a technical stack, but there are many cross-language applications in the front office, AI, Big Data, for example C++/Python/Golang etc. Their service governance capacity is missing due to SDK without a corresponding language.\\n\\nWe note that some concepts of Service Mesh in the cloud are beginning to emerge, so we are beginning to explore this direction.In the concept of Service Mesh, there are two concepts, one Control Plane Control and one Data Plane Dataplane.The core idea of the data plane is to decouple and to abstract some of the unconnected and complex logic (such as service discovery in RPC calls, service routing, melting breaks, security) into an independent process.As long as there is no change in the communications agreement between the operational and independent processes, the evolution of these capabilities can follow the autonomous upgrading of this independent process and the evolution of the entire Mesh can take place in a unified manner.Our cross-language applications, as long as the traffic passes through our Data Plane, are able to enjoy the capacities related to the governance of the services just mentioned, and the application of infrastructure capabilities to the bottom is transparent and truly cloud.\\n\\n## Ant Mesh landing process\\n\\nSo, starting at the end of 2017, ant began to explore the technical direction of Service Mesh and presented a vision of a unified infrastructure with a sense of business upgrade.The main milestone is\uff1a\\n\\nThe Technology Advance Research Service Mesh technology was launched at the end of 2017 and set the direction for the future;\\n\\nBeginning in early 2018 with Golang Self Research Sidecar MOSN and its source, mainly supporting RPC on a two-decimal scale pilot;\\n\\n2019 New Message Mesh and DB Mesh shape in 618, covering a number of core links and exponentially 618\\n\\nTwo-11 years in 2019, covering hundreds of applications from all high-profile core links, supporting the Big Eleven at that time;\\n\\nTwenty and eleven years in 2020, more than 80% of online applications are connected to the Mesh system and can be upgraded from capacity development to full capacity for 2 months.\\n\\n## Ant Mesh Landing Architecture\\n\\nMesh at ant landing size is about thousands of applications and hundreds of thousands of levels of containers, a scale that falls in industry to a few and two times without a previous path to learn, so as ant arrives in a complete system of research and development delivery to support the mesh of ants as he arrives.\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*eAlMT7SMTpMAAAAAAAAAAAAAARQnAQ)\\nAnt Mesh structure is probably our control plane, as shown in the graph, and the service end of the service governance centre, PaaS, monitoring centre, etc. are deployed as some of the existing products.There are also our transport systems, including R&D platforms and PaaS platforms.The middle is our main player data plane MOSN, which manages RPC, messages, MVC, Tasks four streams, as well as basic capabilities for health screening, monitoring, configuration, security, and technical risks, and MOSN blocks some interaction between operations and basic platforms.DBMesh is an independent product in the ant and is not drawn in the graph.Then the top tier is some of our applications that currently support access to many languages such as Java, Nodejs.\\nFor applications, while infrastructure decoupling, access will require an additional upgrade cost, so in order to promote access to the app, ant makes the entire research and development delivery process, including by making the simplest access to the existing framework, by pushing forward in batches to manage risks and progress, and by allowing new applications default access to Mesh to do so.\\n\\nAt the same time, as sincerity grows, each of the capacities faced some problems of collaboration in R&D, and even of mutual impact on performance and stability, so that for the development effectiveness of the Mesh itself, we have made improvements in modular isolation, dynamic plugging of new capacities, automatic regression, and so on, which can be completed within two months from development to roll-out across the site.\\n\\n## Explore on Cloud Native Apps Run\\n\\n**New issues and reflections on mass backwardness**\\n\\nAnt Mesh has now encountered some new problems with\uff1a\\ncross-language SDK maintenance master\uff1aCanada RPC examples. Most of the logic is already sinking into MOSN, but there is still some communication decoding protocol logic in Java, this SDK has some maintenance costs, how many lightweight SDKs, how many languages a team cannot have research and development in all languages. The quality of the Institute\'s code in this lightweight SDK is a problem.\\n\\nA part of the application of the new\uff1aant in business compatible with different environments is deployed both inside the ant and externally exported to financial institutions.When they are deployed to ant the control face of the ant and when the bank is received, the control of the bank is already in place.Most of the applications now contain a layer of their code and temporarily support the next when they meet unsupported components.\\n\\nThe earliest scenes from Service Mesh to Multi-Mesh\uff1aant are Service Mesh, MOSN intercept traffic through network connecting agents, and other intermediates interact with the server through the original SDK.Now MOSN is more than a Service Mosh, but multi-Mesh, because, with the exception of RPC, we have supported more mesh Mesh landing sites, including messages, configurations, caches, etc.Each sinking intermediate can be seen, and almost all have a lightweight SDK on the side of the app, which, in the context of the first issue just a moment ago, finds a very large amount of lightweight SDK that needs to be maintained.In order to keep the features do not interact with each other, each feature opens different ports, calls with MOSN via different protocol.e.g. RPC protocol for RPC, MQ protocol for messages, cached Redis protocol.Then the current MOSN is more than just a flow orientation. For example, the configuration is to expose the API to use business code.\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*80o8SYwyHJoAAAAAAAAAAAAAARQnAQ)\\n \\nTo solve the problems and scenes we are thinking about the following points\uff1a\\n\\nCan the SDK be styled in different intermediaries, languages and languages?\\n\\nCan interoperability protocols be unified?\\n\\n3. Do we sink under our intermediate part to components or capabilities?\\n\\nCan the implementation of the bottom be replaced?\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*hsZBQJg0VnoAAAAAAAAAAAAAARQnAQ)\\n\\n## Ant Cloud Native Apps Runtime Structure\\n\\nBeginning last March, following several rounds of internal discussions and research into new ideas in industry, we introduced a concept of \u201ccloud native apps\u201d (hereinafter referred to as running on).By definition, we want this operation to include all distributive capabilities that the app cares for, help developers build your cloud native apps quickly, help apps and infrastructure to decouple more!\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*iqQoTYAma4YAAAAAAAAAAAAAARQnAQ)\\n\\nThe core points of runtime design for cloud-native applications are as follows:\\n\\n\\\\*\\\\*First \\\\*\\\\*, due to experience of MOSN sizing and associated shipping systems, we decided to build up our cloud native app on the basis of MOSN kernel.\\n\\n\\\\*\\\\*Second \\\\*\\\\*, Abilities instead of Component Orientation, define the APIs for this running time.\\n\\n**Third**, the interaction between business code and the Runtime API uses a uniform gRPC protocol so that the side of the business can generate a client directly and directly call through proto file.\\n\\n**Four**\'s component implementation after ability is replacable, for example, registration service provider may be SOFARegistry, or Nacos or Zookeper.\\n\\n**Running abstract capabilities**\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*hWIVR6ccduYAAAAAAAAAAAAAARQnAQ)\\n\\nTo abstract some of the capabilities most needed for cloud apping, we set a few principles\uff1a\\n\\n1. Follow the APIs and Scenarios required for distributed apps instead of components;\\n   2.APIs are intuitive, used in boxes, and are better than configured;\\n   3.APIs are not bound to implement and differentiate using extension fields.\\n\\nWith this principle, we abstract out the primary API, which is the app for mosn.proto, the appcallback.proto for the app when running, and the relevant actuator.proto for the app when running.For example, RPC calls, messages, read caches, read configurations are all applied to running, while RPC receipts, messages, incoming task schedules, are applied when running. Other control checks, component management, traffic controls are related to running wikes.\\n\\nThree examples of this proto can be seen at\uff1a\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*J76nQoLLYWgAAAAAAAAAAAAAARQnAQ)\\n\\n**Run Component Controls**\\n\\nOn the other hand, we have two concepts in MOSN for the purpose of realizing replaceability when running. We call a distribution capability and then have a different component to perform this Service, a service that can be implemented with multiple components, and a component that can deliver multiple services.For example, the example in the graph is that the service with the message \\"MQ-pub\\" is implemented by SOFAMQ and Kafka Component, while Kafka Component implements both the message and health check service.\\nWhen a transaction is actually requested via a gRPC-generated client, the data will be sent to Runtime via the gRPC protocol and distributed to the next specific implementation.In this way, the app needs to use only the same set of API, which can be implemented differently by the parameters in the request or when the configuration is running.\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*dK9rRLTvtlMAAAAAAAAAAAAAARQnAQ)\\n\\n**Compare between runtime and Mesh**\\n\\nBased on the above, when the cloud app is running and just just Mesh are easy to compare with\uff1a\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*xyu9T74SD9MAAAAAAAAAAAAAARQnAQ)\\n\\nScene\\nstarted research last year while the cloud native app is running. The following scenes are currently falling inside the ant area.\\n\\n**Isomer Technical Stack Access**\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*8UJhRbBg3zsAAAAAAAAAAAAAARQnAQ)\\n\\nIn the case of ants, applications in different languages, in addition to the need for RPC service governance, messages, etc., the infrastructure capabilities such as the one-size-fits-all intermediate of the ant are desirable and Java and Nodejs have corresponding SDKs, while the other languages are not corresponding SDKs.After the application runs, these isomer languages can be used directly through GRPC Client to the ant infrastructure.\\n\\n**Unbind the manufacturer**\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*eVoqRbkTFFwAAAAAAAAAAAAAARQnAQ)\\n\\nAs mentioned earlier, ant blockchains, wind control, intelligent support services, financial intermediaries, etc., are scenes where they are deployed on their main stations, where there is either Aliyun or cloud.After running, the app can combine a set of code with a mirror when running. By configuring it to determine which bottom layer of implementation to be called, without being bound to specific implementations.For example, the internal interface between ant is for products such as SOFARegistration and SOFAMQ, and on the cloud is for products such as Nacos, RocketMQ, to Zokeper, Kafka and others.This scenario is in the process of reaching us.Of course, this can also be used for legacy system governance, such as upgrading from SOFAMQ 1.0 to SOFAMQ 2.0, and then running apps need not be upgraded.\\n\\n\\\\*\\\\*FaaS Cold Pool Pool \\\\*\\\\*\\n\\nFaaS Cool is also a recent scene we are exploring and you know that the Function in FaaS needs to go from Pod creation to Download Function to Start, a process that will be lengthy.After running time, we can create Pod in advance and start up good running. Wait a very simple app logic when the app starts. Test it can be shortened from 5s to 1s.We will continue to explore this direction as well.\\n\\n## Planning and outlook\\n\\n**API**\\n\\nThe most important part of the running time is the definition of the API. We already have a more complete set of APIs for the sake of getting inside, but we also see that many products in industry have similar demands, such as dapr, envoy, etc.So one of the next things we will do is to bring together communities to launch a set of recognized cloud native API.\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*d2BORogVotoAAAAAAAAAAAAAARQnAQ)\\n\\n**Continuous Open Source**\\n\\nWe will also develop our internal running practice in the near future, with a release of 0.1 in May and June, and keep a small monthly release pace, aiming to publish 1.0 by the end of the year.\\n\\n>![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*Kgr9QLc5TH4AAAAAAAAAAAAAARQnAQ)\\n\\n## Summary\\n\\n**Last Summary\uff1a**\\n\\n1.Service Mesh mode introduction is a key path to the application of the cloud;\\n\\nAny mesh that allows Mesh to be generated, but the problem of R&D efficiency remains partially present;\\n\\n3.Mesh Large-scale landfall is a matter of engineering and requires a complete suite of systems;\\n\\n4. Cloud native applications will be the future shape of basic technologies such as intermediaries, further decoupling and distributive capabilities;\\n\\nThe cloud native app runs at the heart of the API, and the community is expected to build one standard together.\\n\\nExtend Reading\\n\\n- [Take you into Cloud Native Technology\uff1aNative Open Delivery Systems Exploration and Practices](https://mp.weixin.qq.com/s?_biz=MzUzU5Mjc1Nw===\\\\&mid=2247488044\\\\&idx=1\\\\&sn=e6300d4b451723a5001cd3deb17fbc\\\\&chksm=faa0f6cdd774e03ccd91300996747a8e7e109ecf810af147e08c663676946490\\\\&scene=21)\\n\\n- [Taking a thousand miles one step at a time: A comprehensive overview of the QUIC protocol landing at Ant Group](https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==\\\\&mid=2247487717\\\\&idx=1\\\\&sn=ca9452cdc10989f61afbac2f012ed712\\\\&chksm=faa0ff3fcdd77629d8e5c8f6c42af3b4ea227ee3da3d5cdf297b970f51d18b8b1580aac786c3\\\\&scene=21)\\n\\n- [Rust\'s emerging field showing its prowess: confidential computing](https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==\\\\&mid=2247487576\\\\&idx=1\\\\&sn=0d0575395476db930dab4e0f75e863e5\\\\&chksm=faa0ff82cdd77694a6fc42e47d6f20c20310b26cedc13f104f979acd1f02eb5a37ea9cdc8ea5\\\\&scene=21)\\n\\n- [Protocol Extension Base on Wasm \u2014 protocol extension](https://mp.weixin.qq.com/s?_biz=MzUzU5Mjc1Nw===\\\\&mid=2247487546\\\\&idx=1\\\\&sn=72c3f1e27ca4ace788e11ca20d5f9\\\\&chksm=faa0ffe0cd776f6d17323466b500acee50a371663f18da34d8e4d72304d7681cf589b45\\\\&scene=21)"},{"id":"/tcpcopy_code_analyze","metadata":{"permalink":"/en-US/blog/tcpcopy_code_analyze","editUrl":"https://github.com/mosn/layotto/edit/main//i18n/en-US/docusaurus-plugin-content-blog/tcpcopy_code_analyze.md","source":"@site/i18n/en-US/docusaurus-plugin-content-blog/tcpcopy_code_analyze.md","title":"Source Parse 4 Layer Traffic Governance, tcp traffic dump","description":"Author profile\uff1a","date":"2024-07-05T07:00:37.000Z","tags":[],"readingTime":3.34,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Ant Cloud Native Apps Exploring and Practice - ArchiSummit","permalink":"/en-US/blog/exploration-and-practice-of-antcloud-native-application-runtime-archsummit-shanghai"},"nextItem":{"title":"MOSN subproject Layotto\uff1aopens the service grid + new chapter when app runs","permalink":"/en-US/blog/mosn-subproject-layotto-opening-a-new-chapter-in-service-grid-application-runtime"}},"content":"> Author profile\uff1a\\n> Giggon, is an open source community lover committed to embracing open sources.\\n>\\n> Writing on: April 26, 2022\\n\\n## Overview\\n\\nThe purpose of this document is to analyze the implementation of tcp traffic dump\\n\\n## Prerequisite\uff1a\\n\\nDocument content refers to the following version of the code\\n\\n[https://github.com/mosn/layotto](https://github.com/mosn/layotto)\\n\\nLayotto 0e97e97e970dc504e0298017bd956d2841c44c0810b (main)\\n\\n## Source analysis\\n\\n### Code in\uff1a [tcpcopy CODE](https://github.com/mosn/layotto/tree/main/pkg/filter/network/tcpcopy)\\n\\n### model.go analysis\\n\\nThis is the core class of tcpcopy\'s configuration objects\\n\\n```go\\nType DumpConfig struct {-\\n\\tSwitch `json:\\"twitch\\"` // dump switch. Values\uff1a\'ON\' or \'OFF\'\\n\\tInterval int `json:\\"interval\\" //dump sampling interval Unit: Second\\n\\tDuration int `json:\\"duration\\"// Single Sampling Cycle Unit: Second\\n\\tCpuMaxate float64 `json:\\"cpu_max_rate\\"\\\\/ cpu Maximum usage The ump feature will stop\\n\\tMemMaxRate float64 `json:\\"mem_max_rate\\"` // mem maximum usage. When this threshold is exceeded, The ump feature will stop\\n}\\n\\nType DumpUpadDynamic Architect 6\\n\\tUnique_sample_windowing string// Specify sample window\\n\\tBusinessType _type. usinessType // Business Type\\n\\tPort string // Port\\n\\tBinary_flow_data []byte// binary data\\n\\tPortrait_data string // User uploaded data\\n}\\n```\\n\\n### persistence.go analysis\\n\\nThis is the dump persistent core processing class of tcpcopy\\n\\n```go\\n// This method is called in OnData in tcpcopy.go\\nfunc IsPersistence() bool {\\n\\t// \u5224\u65ad dump \u5f00\u5173\u662f\u5426\u5f00\u542f\\n\\tif !strategy.DumpSwitch {\\n\\t\\tif log.DefaultLogger.GetLogLevel() >= log.DEBUG {\\n\\t\\t\\tlog.DefaultLogger.Debugf(\\"%s the dump switch is %t\\", model.LogDumpKey, strategy.DumpSwitch)\\n\\t\\t}\\n\\t\\treturn false\\n\\t}\\n\\n\\t// Check whether it is in the sampling window\\n\\tif atomic.LoadInt32(&strategy.DumpSampleFlag) == 0 {\\n\\t\\tif log.DefaultLogger.GetLogLevel() >= log.DEBUG {\\n\\t\\t\\tlog.DefaultLogger.Debugf(\\"%s the dump sample flag is %d\\", model.LogDumpKey, strategy.DumpSampleFlag)\\n\\t\\t}\\n\\t\\treturn false\\n\\t}\\n\\n\\t// Check whether the dump function is stopped. Obtain the system load and check whether the processor and memory exceeds the threshold of the tcpcopy. If yes, stop the dump function.\\n\\tif !strategy.IsAvaliable() {\\n\\t\\tif log.DefaultLogger.GetLogLevel() >= log.DEBUG {\\n\\t\\t\\tlog.DefaultLogger.Debugf(\\"%s the system usages are beyond max rate.\\", model.LogDumpKey)\\n\\t\\t}\\n\\t\\treturn false\\n\\t}\\n\\n\\treturn true\\n}\\n\\n// Persist data based on configuration information\\nfunc persistence(config *model.DumpUploadDynamicConfig) {\\n\\t// 1.Persisting binary data\\n\\tif config.Binary_flow_data != nil && config.Port != \\"\\" {\\n\\t\\tif GetTcpcopyLogger().GetLogLevel() >= log.INFO {\\n\\t\\t\\tGetTcpcopyLogger().Infof(\\"[%s][%s]% x\\", config.Unique_sample_window, config.Port, config.Binary_flow_data)\\n\\t\\t}\\n\\t}\\n\\tif config.Portrait_data != \\"\\" && config.BusinessType != \\"\\" {\\n\\t\\t// 2. Persisting Binary data Persisting user-defined data\\n\\t\\tif GetPortraitDataLogger().GetLogLevel() >= log.INFO {\\n\\t\\t\\tGetPortraitDataLogger().Infof(\\"[%s][%s][%s]%s\\", config.Unique_sample_window, config.BusinessType, config.Port, config.Portrait_data)\\n\\t\\t}\\n\\n\\t\\t// 3. Changes in configuration information in incrementally persistent memory\\n\\t\\tbuf, err := configmanager.DumpJSON()\\n\\t\\tif err != nil {\\n\\t\\t\\tif log.DefaultLogger.GetLogLevel() >= log.DEBUG {\\n\\t\\t\\t\\tlog.DefaultLogger.Debugf(\\"[dump] Failed to load mosn config mem.\\")\\n\\t\\t\\t}\\n\\t\\t\\treturn\\n\\t\\t}\\n\\t\\t// 3.1. dump if the data changes\\n\\t\\ttmpMd5ValueOfMemDump := common.CalculateMd5ForBytes(buf)\\n\\t\\tmemLogger := GetMemLogger()\\n\\t\\tif tmpMd5ValueOfMemDump != md5ValueOfMemDump ||\\n\\t\\t\\t(tmpMd5ValueOfMemDump == md5ValueOfMemDump && common.GetFileSize(getMemConfDumpFilePath()) <= 0) {\\n\\t\\t\\tmd5ValueOfMemDump = tmpMd5ValueOfMemDump\\n\\t\\t\\tif memLogger.GetLogLevel() >= log.INFO {\\n\\t\\t\\t\\tmemLogger.Infof(\\"[%s]%s\\", config.Unique_sample_window, buf)\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\tif memLogger.GetLogLevel() >= log.INFO {\\n\\t\\t\\t\\tmemLogger.Infof(\\"[%s]%+v\\", config.Unique_sample_window, incrementLog)\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n```\\n\\n### tcpcopy.go analysis\\n\\nThis is the core class of tcpcopy.\\n\\n```go\\n// Sign up to NetWork \\nfunc init() with MFA\\n\\tapi. egisterNetwork(\\"tcpcopy\\", CreateTccopyFactory)\\n}\\n\\n// returns tcpcopy Factory\\nfunc CreateTccopyFactory(cfg map[string]interface{}) (api. etworkFilterChainFactory, error) LO\\n\\ttcpConfig := &config{}\\n\\t// dump policy transition to static configuration\\n\\tif stg, ok := cfg[\\"strategy\\"]; ok {\\n\\t...\\n\\t}\\n\\t//TODO excerpt some other fields\\n\\treturn &tcpcopyFactoryLU\\n\\t\\tcfg: tcpConfig,\\n\\t}, nil\\n}\\n\\n// for pkg/configmanager/parser. o Call to add or update Network filter factory\\nfunc (f *tcpcopyFactory) Init(param interface{}) error error 56\\n\\t// Set listening address and port configuration\\n\\t...\\n\\treturn nil\\n}\\n\\n// implements the OnData Interface of ReadFilter, processing\\nfunc (f *tcpcopyFactory) OnData(data types.IoBuffer) (res api. ilterStatus) online\\n\\t// Determines whether the current requested data requires sampling dump \\n\\tif !persiste.Isistence() {\\n\\t\\treturn api.Continue\\n\\t}\\n\\n\\t// Asynchronous sample dump\\n\\tconfig := model.NewDumpUpadDynamic Config(strategy. umpSampleUuid, \\"\\", f.cfg.port, data.Bytes(), \\"\\")\\n\\tpersistence.GetDumpWorkPoolInstance().Schedule(config)\\n\\treturn api.Continue\\n}\\n```\\n\\nFinally, we look back at the overall process progress:\\n\\n1. Starting from the initialization function init() of tccopy.go to CreateGRPCServerFilterFactory Incoming CreateTcpcopyFactory.\\n\\n2. Mosn created a filter chain (code position[factory.go](https://github.com/mosn/mosn/tree/master/pkg/filter/network/proxy/factory.go)) by circulating CreateFilterChain to add all filters to the chain structure, including tccopy.\\n\\n3. When the traffic passes through mosn will enter the tcpcopy.go OnData method for tcpcopump logical processing."},{"id":"/mosn-subproject-layotto-opening-a-new-chapter-in-service-grid-application-runtime","metadata":{"permalink":"/en-US/blog/mosn-subproject-layotto-opening-a-new-chapter-in-service-grid-application-runtime","editUrl":"https://github.com/mosn/layotto/edit/main//i18n/en-US/docusaurus-plugin-content-blog/mosn-subproject-layotto-opening-a-new-chapter-in-service-grid-application-runtime/index.md","source":"@site/i18n/en-US/docusaurus-plugin-content-blog/mosn-subproject-layotto-opening-a-new-chapter-in-service-grid-application-runtime/index.md","title":"MOSN subproject Layotto\uff1aopens the service grid + new chapter when app runs","description":"Author profile\uff1a","date":"2024-07-05T07:00:37.000Z","tags":[],"readingTime":20.975,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Source Parse 4 Layer Traffic Governance, tcp traffic dump","permalink":"/en-US/blog/tcpcopy_code_analyze"},"nextItem":{"title":"Source Parsing 7 Layer Traffic Governance, Interface Limit","permalink":"/en-US/blog/code/flowcontrol/flowcontrol_code_analyze"}},"content":"> Author profile\uff1a\\n> Magnetic Army. Fancy is an ancient one, cultivating for many years in the infrastructure domain, with in-depth practical experience of Service Mosh, and currently responsible for the development of projects such as MOSN, Layotto and others in the middle group of ant groups.\\n> Layotto official GitHub address: [https://github.com/mosn/layotto](https://github.com/mosn/layotto)\\n\\nClick on a link to view the live video\uff1a[https://www.bilibili.com/video/BV1hq4y1L7FY/](https://www.bilibili.com/video/BV1hq4y1L7FY/)\\n\\nService Mesh is already very popular in the area of microservices, and a growing number of companies are starting to fall inside, and ants have been investing heavily in this direction from the very beginning of Service Mesh programme. So far, the internal Mesh programme has covered thousands of applications, hundreds of thousands of containers and has been tested many times, the decoupling of business coupling brought about by Service Mosh, smooth upgrades and other advantages have greatly increased iterative efficiency in intermediaries.\\n\\nWe have encountered new problems after mass landings, and this paper focuses on a review of service Mesh\'s internal landings and on sharing solutions to new problems encountered after service Mesh landing.\\n\\n## Service Mesh Review and Summary\\n\\n### Instrument for standardized international reporting of military expenditures\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*p8tGTbpLRegAAAAAAAAAAAAAARQnAQ)\\n> Under the microservice architecture, infrastructure team typically provides a SDK that encapsulates the ability to govern the various services, while ensuring the proper functioning of the application, it is also clear that each infrastructure team iterates a new feature that requires the involvement of the business party to use it, especially in the bug version of the framework, often requiring a forceful upgrade of the business side, where every member of the infrastructure team has a deep sense of pain.\\n\\nThe difficulties associated with upgrading are compounded by the very different versions of the SDK versions used by the application and the fact that the production environment runs in various versions of the SDK, which in turn makes it necessary to consider compatibility for the iterations of new functions as if they go ahead with the shacks, so that the maintenance of the code is very difficult and some ancestral logic becomes uncareful.\\n\\nThe development pattern of the \u201cheavy\u201d SDKs makes the governance of the isomer language very weak and the cost of providing a functionally complete and continuously iterative SDK for all programming languages is imaginable.\\n\\nIn 18 years, Service Mesh continued to explode in the country, a framework concept designed to decouple service governance capacity with business and allow them to interact through process-level communications.Under this architecture model, service governance capacity is isolated from the application and operated in independent processes, iterative upgrading is unrelated to business processes, which allows for rapid iterations of service governance capacity, and each version can be fully upgraded because of the low cost of upgrading, which has addressed the historical burden and the SDK \u201clight\u201d directly reduces the governance threshold for isomer languages and no longer suffers from the SDK that needs to develop the same service governance capability for each language.\\n\\n### Current status of service Mesh landings\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*rRG_TYlHMqYAAAAAAAAAAAAAARQnAQ)\\n> \u8682\u8681\u5f88\u5feb\u610f\u8bc6\u5230\u4e86 Service Mesh \u7684\u4ef7\u503c\uff0c\u5168\u529b\u6295\u5165\u5230\u8fd9\u4e2a\u65b9\u5411\uff0c\u7528 Go \u8bed\u8a00\u5f00\u53d1\u4e86 MOSN \u8fd9\u6837\u53ef\u4ee5\u5bf9\u6807 envoy \u7684\u4f18\u79c0\u6570\u636e\u9762\uff0c\u5168\u6743\u8d1f\u8d23\u670d\u52a1\u8def\u7531\uff0c\u8d1f\u8f7d\u5747\u8861\uff0c\u7194\u65ad\u9650\u6d41\u7b49\u80fd\u529b\u7684\u5efa\u8bbe\uff0c\u5927\u5927\u52a0\u5feb\u4e86\u516c\u53f8\u5185\u90e8\u843d\u5730 Service Mesh \u7684\u8fdb\u5ea6\u3002\\n\\nNow that MOSN has overwritten thousands of apps and hundreds of thousands of containers inside ant ants, newly created apps have default access to MOSN to form closers.And MOSN handed over a satisfactory\uff1a in terms of resource occupancy and loss of performance that is of greatest concern to all.\\n\\n1. RT is less than 0.2 ms\\n\\n2. Increase CPU usage by 0% to 2%\\n\\n3. Memory consumption growth less than 15M\\n\\nThe technical stack of the NodeJS, C+++ isomers is also continuously connected to MOSN due to the Service Mesh service management thresholds that lower the isomer language.\\n\\nAfter seeing the huge gains from RPC capacity Mih, internal ants also transformed MQ, Cache, Config and other middleware capabilities, sinking to MOSN, improving the iterative efficiency of the intermediate product as a whole.\\n\\n### C. New challenges\\n\\n1. Apply strong binding to infrastructure\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*nKxcTKLp4EoAAAAAAAAAAAAAARQnAQ)\\n> A modern distributed application often relies on RPC, Cache, MQ, Config and other distributed capabilities to complete the processing of business logic.\\n\\nWhen RPC was initially seen, other capabilities were quickly sinking.Initially, they were developed in the most familiar way, leading to a lack of integrated planning management, as shown in the graph above, which relied on SDKs of a variety of infrastructure, and in which SDK interacted with MOSN in a unique way, often using private agreements provided by the original infrastructure, which led directly to a complex intermediate capability, but in essence the application was tied to the infrastructure, such as the need to upgrade the SDK from Redis to Memcache, which was more pronounced in the larger trend of the application cloud, assuming that if an application was to be deployed on the cloud, because the application relied on a variety of infrastructures, it would be necessary to move the entire infrastructure to the cloud before the application could be successfully deployed.\\nSo how to untie the application to the infrastructure so that it can be transplantable and that it can feel free to deploy across the platform is our first problem.\\n\\n2. Isomal language connectivity\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*oIdQQZmgtyUAAAAAAAAAAAAAARQnAQ)\\n> It has been proved that Service Mesh does reduce the access threshold of heterogeneous languages, but after more and more basic capabilities sink to MOSN, we gradually realized that in order to allow applications to interact with MOSN, various SDKS need to develop communication protocols and serialization protocols. If you add in the need to provide the same functionality for a variety of heterogeneous languages, the difficulty of maintenance increases exponentially\\n\\nService Mesh has made the SDK historic, but for the current scenario of programming languages and applications with strong infrastructural dependence, we find that the existing SDK is not thin enough, that the threshold for access to the isomer language is not low enough and that the threshold for further lowering the isomer language is the second problem we face.\\n\\n## Multi Runtime Theory Overview\\n\\n### A, what is Runtime?\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*hQT-Spc5rI4AAAAAAAAAAAAAARQnAQ)\\n> In the early 20th century, Bilgin lbryam published a paper called\\n> Multi-Runtime Microservices Architecture\\n> This article discusses the shape of the next phase of microservices architecture.\\n\\nAs shown in the graph above, the author abstracts the demand for distributed services and is divided into four chaos\uff1a\\n\\n1. Life Cycle (Lifecycle)\\n   mainly refers to compilation, packing, deployment and so forth, and is largely contracted by docker and kubernetes in the broad cloud of origins.\\n\\n2. Network (Networking)\\n   A reliable network is the basic guarantee of communication between microservices, and Service Mesh is trying to do so and the stability and usefulness of the current popular data face of MOSN and envoy have been fully tested.\\n\\n3. The status (State)\\n   services that are required for distribution systems, workflow, distribution single, dispatching, power equivalent, state error restoration, caching, etc. can be uniformly classified as bottom status management.\\n\\n4. Binding (Binding)\\n   requires not only communication with other systems but also integration of various external systems in distributed systems, and therefore has strong reliance on protocol conversion, multiple interactive models, error recovery processes, etc.\\n\\nAfter the need has been clarified, drawing on the ideas of Service Mesh, the author has summarized the evolution of the distributed services architecture as\uff1a below.\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*rwS2Q5yMp_sAAAAAAAAAAAAAARQnAQ)\\n> Phase I is to decouple infrastructure capabilities from the application and to convert them into an independent residecar model that runs with the application.\\n\\nThe second stage is to unify the capabilities offered by the sidecar into a single settlement run from the development of the basic component to the development of the various distributive capabilities to the development of the various distributive capacities, completely block the details of the substrate and, as a result of the ability orientation of the API, the application no longer needs to rely on SDK from a wide range of infrastructures, except for the deployment of the APIs that provide the capabilities.\\n\\nThe author\'s thinking is consistent with what we want to resolve, and we have decided to use the Runtime concept to solve the new problems that Service Mesh has encountered to date.\\n\\n### B, Service Mesh vs Runtime\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*srPVSYTEHc4AAAAAAAAAAAAAARQnAQ)\\n> In order to create a clearer understanding of Runtime, a summary of Service Mesh with regard to the positioning, interaction, communication protocols and capacity richness of the two concepts of Runtime is shown, as can be seen from Service Mosh, when Runtime provides a clearly defined and capable API, making the application more straightforward to interact with it.\\n\\n## MOSN sub-project Layotto\\n\\n### A, dapr research\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*Ab9HSYIK7CQAAAAAAAAAAAAAARQnAQ)\\n> dapr is a well-known Runtime product in the community and has a high level of activity, so we first looked at the dapr case, finding that the dapr has the following advantage of\uff1a\\n\\n1. A variety of distributive capabilities are provided, and the API is clearly defined and generally meets the general usage scenario.\\n\\n2. Different delivery components are provided for each capability, essentially covering commonly used intermediate products that can be freely chosen by users as needed.\\n\\nWhen considering how to set up a dapr within a company, we propose two options, such as the chart\uff1a above\\n\\n1. Replace\uff1awith the current MOSN and replace with the dapr. There are two problems with\uff1a\\n\\nDapr does not currently have the full range of service governance capabilities included in Service Mesh although it provides many distributive capabilities.\\n\\nb. MOSN has fallen on a large scale within the company and has been tested on numerous occasions with the direct replacement of MOSN stability by a dapr.\\n\\n2. In\uff1a, add a dapr container that will be deployed with MOSN in two sidecar mode.This option also has two problems with\uff1a\\n\\nThe introduction of a new sidecar will require consideration of upgrading, monitoring, infusion and so forth, and the cost of transport will soar.\\n\\nb. The increased maintenance of a container implies an additional risk of being hacked and this reduces the availability of the current system.\\n\\nSimilarly, if you are currently using envoy as a data face, you will also face the above problems.\\nWe therefore wish to combine Runtime with Service Mesh and deploy through a full sidecar to maximize the use of existing MSh capabilities while ensuring stability and the constant cost of delivery.In addition, we hope that, in addition to being associated with MOSN, the capacity of the RPF will be combined in the future with envoy to solve the problems in more scenarios, in which Layotto was born.\\n\\n### Layout B & Layout\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*sdGoSYB_XFUAAAAAAAAAAAAAARQnAQ)\\n> As shown in the above chart, Layotto is above all over the infrastructure and provides a standard API for upper-tier applications with a uniform range of distributive capabilities.For Layotto applications, developers no longer need to care for differences in the implementation of substrate components, but just what competencies the app needs and then call on the adaptive API, which can be completely untied to the underlying infrastructure.\\n\\nFor applications, interaction is divided into two blocks, one as a standard API for GRPC Clients calling Layotto and another as a GRPC Server to implement the Layotto callback and benefit from the gRPC excellent cross-language support capability, which no longer requires attention to communications, serialization, etc., and further reduces the threshold for the use of the technical stack of isomers.\\n\\nIn addition to its application-oriented, Layotto also provides a unified interface to the platform that feeds the app along with the sidecar state of operation, facilitates SRE peer learning to understand the state of the app and make different initiatives for different states, taking into account existing platform integration with k8s and so we provide access to HTTP protocol.\\n\\nIn addition to Layotto itself design, the project involves two standardized constructions, firstly to develop a set of terminological clocks; the application of a broad range of APIs is not an easy task. We have worked with the Ari and Dapr communities in the hope that the building of the Runtime API will be advanced, and secondly for the components of the capabilities already achieved in the dapr community, our principle is to reuse, redevelop and minimize wasting efforts over existing components and repeat rotations.\\n\\nIn the end, Layotto is now built over MOSN, we would like Layotto to be able to run on envoy, so that you can increase Runtime capacity as long as you use Service Mesh, regardless of whether the data face is used by MOSN or envoy.\\n\\n### C, Layotto transplantation\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*2DrSQJ6GL8cAAAAAAAAAAAAAARQnAQ)\\n> As shown in the graph above, once the standardisation of the Runtime API is completed, access to Layotto applications is naturally portable, applications can be deployed on private clouds and various public clouds without any modification, and since standard API is used, applications can be freely switched between Layotto and dapr without any modification.\\n\\n### Meaning of name\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*CCckTZ_gRsMAAAAAAAAAAAAAARQnAQ)\\n> As can be seen from the above schematic chart, the Layotto project itself is intended to block the details of the infrastructure and to provide a variety of distributive capabilities to the upper level of application. This approach is as if it adds a layer of abstraction between the application and the infrastructure, so we draw on the OSI approach to defining a seven-tiered model of the network and want Layot to serve the eighth tier of the application, to be 8 in Italian, Layer otto is meant to simplify to become Layotto, along with Project Code L8, which is also the eighth tier and is the source of inspiration for\\n\\nAn overview of the completion of the project is presented below, with details of the achievement of four of its main functions.\\n\\n### E. Configuration of original language\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*mfkRQZH3oNwAAAAAAAAAAAAAARQnAQ)\\n> First is the configuration function commonly used in distributed systems, applications generally use the configuration center to switch or dynamically adjust the running state of the application.The implementation of the configuration module in Layotto consists of two parts. One is a reflection on how to define the API for this capability, and one is a specific implementation, each of which is seen below.\\n\\nIt is not easy to define a configuration API that meets most of the actual production demands. Dapr currently lacks this capability, so we worked with Ali and the Dapr community to engage in intense discussions on how to define a version of a reasonable configuration API.\\n\\nAs the outcome of the discussions has not yet been finalized, Layotto is therefore based on the first version of the draft we have submitted to the community, and a brief description of our draft is provided below.\\n\\nWe first defined the basic element\uff1a for general configuration\\n\\n1. appId\uff1aindicates which app the configuration belongs to\\n\\n2. Key configured for key\uff1a\\n\\n3. Value of content\uff1aconfiguration\\n\\n4. group\uff1aconfigurations are configured. If an appId is too many configurations, we can group these configurations for maintenance.\\n\\nIn addition, we added two advanced features to suit more complex configurations using Scene\uff1a\\n\\n1. label, used to label configurations, such as where the configuration belongs, and when conducting configuration queries, we\'ll use label + key to query configuration.\\n\\n2. tags, users give configuration additional information such as description, creator information, final modification time, etc. to facilitate configuration management, audits, etc.\\n\\nFor the specific implementation of the configuration API as defined above, we currently support query, subscription, delete, create, and modify five kinds of actions in which subscriptions to configuration changes use the stream feature of GRPC and the components where the configuration capacity is implemented at the bottom, we have selected the domestically popular apollo and will add others later depending on demand.\\n\\n### F: Pub/Sub\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*YJs-R6WFhkgAAAAAAAAAAAAAARQnAQ)\\n> for Pub/Sub capabilities, we have explored the current implementation of dapr and have found that we have largely met our needs, so we have directly reintroduced the Dapr API and components that have been suitably matched in Layotto, which has saved us a great deal of duplication and we would like to maintain a collaborative approach with the dapr community rather than repeat the rotation.\\n\\nPub is an event interface provided by the App calls Layotto and the Sub function is one that implements ListTopicSubscriptions with OnTopicEvent in the form of a gRPC Server, one that tells Layotto apps that need to subscribe to which topics, and a callback event for Layotto receive a change in top.\\n\\nDapr for the definition of Pub/Sub basically meets our needs, but there are still shortfalls in some scenarios, dapr uses CloudEvent standards, so the pub interface does not return value, which does not meet the need in our production scenes to require pub messages to return to the messageID that we have already submitted the needs to dapr communities, and we are waiting for feedback, taking into account mechanisms for community asynchronous collaboration, we may first increase the results and then explore with the community a better compatibility programme.\\n\\n### G and RPC original\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*i-JnSaeZbJ4AAAAAAAAAAAAAARQnAQ)\\n> The capacity of RPC is not unfamiliar and may be the most basic needs under the microservice architecture, the definition of RPC interfaces, we also refer to the dapr community definition and therefore the interface definition is fully responsive to our needs and thus the interface definition is a direct reuse of dapr but the current RPC delivery programme provided by dapr is still weak, and MOSN is very mature over the years, This is a brave combination of Runtime with Service Mesh and MOSN itself as a component of our capacity to implement RPC and thereby Layotto submit to MOSN for actual data transfer upon receipt of RPC requests, The option could change routing rules through istio, downgraded flow and so on, which would amount to a direct replication of Service Mesh\'s capabilities. This would also indicate that Runtime is not about listing the Service Mesh, but rather a step forward on that basis.\\n\\nIn terms of details, in order to better integrate with MOSN, we have added one Channel, default support for dubbo, bolt, HTTP three common RPC protocols to the RPC. If we still fail to meet the user scene, we have added Before/After filter to allow users to customize extensions and implement protocol conversions, etc.\\n\\n### H, Actuator\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*E_Q-T4d_bm4AAAAAAAAAAAAAARQnAQ)\\n> In actual production environments, in addition to the various distributive capabilities required for the application, we often need to understand the operational state of the application, based on this need, we abstract an actuator interface, and we currently do not have the capability to do so at the moment, dapr and so we are designed on the basis of internal demand scension scenarios to expose the full range of information on the application at the startup and running stages, etc.\\n\\nLayotto divides the exposure information into an individual\uff1a\\n\\n1. Health\uff1aThis module determines whether the app is healthy, e.g. a strongly dependent component needs to be unhealthy if initialization fails, and we refer to k8s for the type of health check to\uff1a\\n\\na. Readiness\uff1aindicates that the app is ready to start and can start processing requests.\\n\\nb. Liveness\uff1aindicates the state of life of the app, which needs to be cut if it does not exist.\\n\\n2. Info\uff1aThis module is expected to expose some of the dependencies of the app, such as the service on which the app depends, the subscription configuration, etc. for troubleshooting issues.\\n\\nHealth exposure health status is divided into the following Atlash\uff1a\\n\\n1. INIT\uff1aindicates that the app is still running. If the app returns this value during the release process, the PaaS platform should continue waiting for the app to be successfully started.\\n\\n2. UP\uff1aindicates that the app is starting up normally, and if the app returns this value, the PasS platform can start loading traffic.\\n\\n3. DOWN\uff1aindicates that the app failed to boot, meaning PaaS needs to stop publishing and notify the app owner if the app returns this value during the release process.\\n\\nThe search for Layotto is now largely complete in the Runtime direction, and we have addressed the current problems of infrastructure binding and the high cost of isomer language access using a standard interactive protocol such as gRPC to define a clearly defined API.As the future API standardises the application of Layotto can be deployed on various privately owned and publicly owned clouds, on the one hand, and free switching between Layotto, dapr and more efficient research and development, on the other.\\n\\nCurrently, Serverless fields are also flown and there is no single solution, so Layotto makes some attempts in Serverless directions, in addition to the input in the Runtime direction described above.\\n\\n## Exploring WebAssembly\\n\\n### Introduction to the Web Assembly\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*-ACRSpqbuJ0AAAAAAAAAAAAAARQnAQ)\\n> WebAssembly, abbreviated WASM, a collection of binary commands initially running on the browser to solve the JavaScript performance problems, but due to its good safety, isolation, and linguistic indifference, one quickly starts to get it to run outside the browser. With the advent of the WASI definition, only one WASM will be able to execute the WAS document anywhere.\\n\\nSince WebAssembly can run outside the browser, can we use it in Serverless fields?Some attempts had been made in that regard, but if such a solution were to be found to be a real one, it would be the first question of how to address the dependence of a functioning Web Assembly on infrastructure.\\n\\n### Principles of B and Web Assembly landing\\n\\nCurrently MOSN runs on MOSN by integrating WASM Runtime to meet the need for custom extensions to MOSN.Layotto is also built over MOSN so we consider combining the two in order to implement the following graph\uff1a\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*U7UDRYyBOvIAAAAAAAAAAAAAARQnAQ)\\n> Developers can develop their code using a variety of preferred languages such as Go/C+/Rust and then run them over MOSN to produce WASM files and call Layotto provide standard API via local function when WASM style applications need to rely on various distribution capabilities in processing requests, thereby directly resolving the dependency of WASM patterns.\\n\\nLayotto now provides Go with the implementation of the Rust version of WASM, while supporting the demo tier function only, is enough for us to see the potential value of such a programme.\\n\\nIn addition, the WASM community is still in its early stages and there are many places to be refined, and we have submitted some PRs to the community to build up the backbone of the WASM technology.\\n\\n### C. WebAssembly Landscape Outlook\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*NzwKRY2GZPcAAAAAAAAAAAAAARQnAQ)\\n> Although the use of WAS in Layotto is still in the experimental stage, we hope that it will eventually become a service unless it is developed through a variety of programming languages, as shown in the graph above, and then codify the WASM document, which will eventually run on Layotto+MOSN, while the application wiki management is governed by k8, docker, prometheus and others.\\n\\n## Community planning\\n\\nFinally, look at what Layotto does in the community.\\n\\n### A, Layotto vs Dapr\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*OpQTRqoMpK0AAAAAAAAAAAAAARQnAQ)\\n> Charted Layotto in contrast to the existing capabilities in Layotto, our development process at Layotto, always aim to achieve the goal of a common building, based on the principle of re-use, secondary development, and for the capacity being built or to be built in the future, we plan to give priority to Layotto and then to the community to merge into standard API, so that in the short term it is possible that the Layotto API will take precedence over the community, but will certainly be unified in the long term, given the mechanism for community asynchronous collaboration.\\n\\n### The APP\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*nKxcTKLp4EoAAAAAAAAAAAAAARQnAQ)\\n> We have had extensive discussions in the community about how to define a standard API and how Layotto can run on envoy, and we will continue to do so.\\n\\n### C, Road map\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*iUV3Q7S3VLEAAAAAAAAAAAAAARQnAQ)\\n> Layotto currently support four major functionalities in support of RPC, Config, Pub/Sub, Actuator and is expected to devote attention to distribution locks and observations in September, and Layotto plugging in December, which it will be able to run on envoy, with the hope that further outputs will be produced for the WebCongress exploration.\\n\\n### Official open source\\n\\n> ![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*S6mdTqAapLQAAAAAAAAAAAAAARQnAQ)\\n> gave a detailed presentation of the Layotto project and most importantly the project is being officially opened today as a sub-project of MOSN and we have provided detailed documentation and demo examples to facilitate quick experience.\\n\\nThe construction of the API standardization is a matter that needs to be promoted over the long term, while standardization means not meeting one or two scenarios, but the best possible fitness for most use scenarios, so we hope that more people can participate in the Layotto project, describe your use scenario, discuss the API definition options, come together to the community, ultimately reach the ultimate goal of Write once, Run any!"},{"id":"/code/flowcontrol/flowcontrol_code_analyze","metadata":{"permalink":"/en-US/blog/code/flowcontrol/flowcontrol_code_analyze","editUrl":"https://github.com/mosn/layotto/edit/main//i18n/en-US/docusaurus-plugin-content-blog/code/flowcontrol/flowcontrol_code_analyze.md","source":"@site/i18n/en-US/docusaurus-plugin-content-blog/code/flowcontrol/flowcontrol_code_analyze.md","title":"Source Parsing 7 Layer Traffic Governance, Interface Limit","description":"Author Profile\uff1a","date":"2024-07-05T07:00:37.000Z","tags":[],"readingTime":1.475,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"MOSN subproject Layotto\uff1aopens the service grid + new chapter when app runs","permalink":"/en-US/blog/mosn-subproject-layotto-opening-a-new-chapter-in-service-grid-application-runtime"},"nextItem":{"title":"Layotto Source Parsing \u2014 Processing RPC requests","permalink":"/en-US/blog/code/layotto-rpc"}},"content":"> Author Profile\uff1a\\n> was a fester of an open source community committed to embracing open sources and hoping to interact with each other\u2019s open-source enthusiasts for progress and growth.\\n>\\n> Writing Time: 20 April 2022\\n\\n## Overview\\n\\nThe purpose of this document is to analyze the implementation of the interface flow\\n\\n## Prerequisite\uff1a\\n\\nDocument content refers to the following version of the code\\n\\n[https://github.com/mosn/mosn](https://github.com/mosn/mosn)\\n\\nMosn d11b5a638a137045c2fb03d9d8ca36ecc0def11 (Division Develop)\\n\\n## Source analysis\\n\\n### Overall analysis\\n\\nReference to <br />[https://mosn.io/docs/concept/extensions/](https://mosn.io/docs/concept/extensions/)\\n\\nMosn Stream Filter Extension\\n\\n![01.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*tSn4SpIkAa4AAAAAAAAAAAAAARQnAQ)\\n\\n### Code in\uff1a [flowcontrol\u4ee3\u7801](https://github.com/mosn/mosn/tree/master/pkg/filter/stream/flowcontrol)\\n\\n### stream_filter_factory.go analysis\\n\\nThis class is a factory class to create StreamFilter.\\n\\nSome constant values are defined for default values\\n\\n![02.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*PAWCTL6MS40AAAAAAAAAAAAAARQnAQ)\\n\\nDefines the restricted stream config class to load yaml definition and parse production corresponding functions\\n\\n![03.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*Ua32SokhILEAAAAAAAAAAAAAARQnAQ)\\n\\ninit() Inner initialization is the storage of name and corresponding constructor to the filter blocking plant map\\n\\n![04.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*kb3qRqWnqxYAAAAAAAAAAAAAARQnAQ)\\n\\nHighlight createRpcFlowControlFilterFactory Production rpc Current Factory\\n\\n![05.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*u5rkS54zkgAAAAAAAAAAAAAAARQnAQ)\\n\\nBefore looking at streamfilter, we see how factory classes are producing restricted streamers\\n\\n![06.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*cj0nT5O69OYAAAAAAAAAAAAAARQnAQ)\\n\\nLimit the streaming to the restricted stream chain structure to take effect in sequential order.\\n\\nCreateFilterChain method adds multiple filters to the link structure\\n\\n![07.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*a8ClQ76odpEAAAAAAAAAAAAAARQnAQ)\\n\\nWe can see that this interface is achieved by a wide variety of plant types, including those that we are studying today.\\n\\n![08.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*sBDbT44r2vgAAAAAAAAAAAAAARQnAQ)\\n\\n### Stream_filter.go Analysis\\n\\n![09.png](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*wsw3RKe1GH8AAAAAAAAAAAAAARQnAQ)\\n\\n## Overall process\uff1a\\n\\nFinally, we look back at the overall process progress:\\n\\n1. Starting from the initialization function of stream_filter_factory.go, the program inserted createRpcFlowControlFilterFactory.\\n\\n2. Mosn created a filter chain (code position[factory.go](https://github.com/mosn/mosn/tree/master/pkg/streamfilter/factory.go)) by circulating CreateFilterChain to include all filters in the chain structure, including our master restricted streaming today.\\n\\n3. Create Limiter NewStreamFilter().\\n\\n4. OnReceive() and eventually by sentinel (whether the threshold has been reached, whether to release traffic or stop traffic, StreamFilterStop or StreamFilterContinue)."},{"id":"/code/layotto-rpc","metadata":{"permalink":"/en-US/blog/code/layotto-rpc","editUrl":"https://github.com/mosn/layotto/edit/main//i18n/en-US/docusaurus-plugin-content-blog/code/layotto-rpc/index.md","source":"@site/i18n/en-US/docusaurus-plugin-content-blog/code/layotto-rpc/index.md","title":"Layotto Source Parsing \u2014 Processing RPC requests","description":"This paper is based on the Dubbo Json RPC as an example of the Layotto RPC processing.","date":"2024-07-05T07:00:37.000Z","tags":[],"readingTime":20.73,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Source Parsing 7 Layer Traffic Governance, Interface Limit","permalink":"/en-US/blog/code/flowcontrol/flowcontrol_code_analyze"},"nextItem":{"title":"Source parsing layotto startup process","permalink":"/en-US/blog/code/start_process/start_process"}},"content":"> This paper is based on the Dubbo Json RPC as an example of the Layotto RPC processing.\\n>\\n> by\uff1a[Wang Zhilong](https://github.com/rayowang) | 21April 2022\\n\\n- [overview](#overview)\\n- [source analysis](#source analysis)\\n  - [0x00 Layotto initialize RPC](#_0x00-layotto-initializ-rpc)\\n  - [0x01 Dubbo-go-sample client to request request] (#_0x01-dubbo-go-sample-client-request request)\\n  - [0x02 Mosn EventLoop Reader Processing Request Data](#_0x02-mosn-eventloop-read processing request)\\n  - [0x03 Grpc Sever as NetworkFilter to process requests](#_0x03-grpc-sever as -networkfilter-process requests)\\n  - [0x04 Layotto send RPC requests and write to Local Virtual Connections](#_0x04-layotto -rpc-request and write to -local-virtual connection)\\n  - [0x05 Mosn reads Remote and executes Filter and Proxy Forwarding](#_0x05-mosn-read-remote-remote--and --filter-and proxy forward)\\n  - [0x06 Dubbo-go-sample server received response to request return] (#_0x06-dubbo-go-sample-server-received response return)\\n  - [0x07 Mosn Framework handles response and writes back to Remote Virtual Connections](#_0x07-mosn-Framework handles response and -remote-virtual connection)\\n  - [0x08 Layotto receive RPC responses and read Local Virtual Connections](#_0x08-layotto-receive-rpc-response and read -local-virtual connection)\\n  - [0x09 Grpc Sever processed data frames returned to client](#_0x09-grpc-sever processed frame returned to client)\\n  - [0x10 Dubbo-go-sample client receiving response](#_0x10-dubbo-go-sample-client-receiving response)\\n- [summary](#summary)\\n\\n## General description\\n\\nLayotto has a clear and rich semiconductor API as a distributed prototype collection of prototype language distinguished from the network proxy service Mesh and using standard protocol API, which is part of the RPC API.Through RPC API app developers can interact with local Layotto instances of applications that also use the Sidecar architecture, thereby indirectly calling different service methods and using built-in capabilities to perform distributive tracking and diagnosis, traffic control, error handling, secure links, etc.and Layotto is based on the Grpc handler design, using the X-Protocol protocol for secure and reliable communications, except for Http/Grpc communications with other services.As shown in the following code, the RPC API interface is in line with Dapr and is available for RPC calls through the Grpc interface InvokeService.\\n\\n```go\\ntype DaprClient interface {\\n    // Invokes a method on a remote Dapr app.\\n    InvokeService(ctx context.Context, in *InvokeServiceRequest, opts ...grpc.CallOption) (*v1.InvokeResponse, error)\\n    ...\\n}\\n```\\n\\n## Source analysis\\n\\nFor ease of understanding, from outside to inside, from inside to outside, from flow to source code, that is, from Client, through one layer of logic to the Server receiving a return response to requests, from another layer of return to client, and from one layer of analysis of Layotto RPC processes, split into 10 steps.Also, since the content of Gypc Client and Server handshakes and interactions is not the focus of this paper, the analysis is relatively brief and the other steps are relatively detailed and one can move directly from the directory to the corresponding step depending on his or her case.\\n\\nNote\uff1abased on commit hash\uff1a1d2bed68c3b2372c34a12aeed41be125a4fdd15a\\n\\n### 0x00 Layotto initialize RPC\\n\\nLayotto starts the process involves a large number of processes in which only the initialization of the process related to RPC and described below is analyzed because Layotto is based on Mosn and is therefore starting from the Main function, urfave/cli library calls Mosn StageManager Mos, thus initializing GrpcServer in Mosn NetworkFilter as follows.\\n\\n```go\\nmosn.io/mosn/pkg/stagemanager.(*StageManager).runInitStage at stage_manager.go\\n=>\\nmosn.io/mosn/pkg/mosn.(*Mosn).initServer at mosn.go\\n=>\\nmosn.io/mosn/pkg/filter/network/grpc.(*grpcServerFilterFactory).Init at factory.go\\n=>\\nmosn.io/mosn/pkg/filter/network/grpc.(*Handler).New at factory.go\\n// \u65b0\u5efa\u4e00\u4e2a\u5e26\u6709\u5730\u5740\u7684 Grpc \u670d\u52a1\u5668\u3002\u540c\u4e00\u4e2a\u5730\u5740\u8fd4\u56de\u540c\u4e00\u4e2a\u670d\u52a1\u5668\uff0c\u53ea\u80fd\u542f\u52a8\u4e00\u6b21\\nfunc (s *Handler) New(addr string, conf json.RawMessage, options ...grpc.ServerOption) (*registerServerWrapper, error) {\\n    s.mutex.Lock()\\n    defer s.mutex.Unlock()\\n    sw, ok := s.servers[addr]\\n    if ok {\\n        return sw, nil\\n    }\\n    ln, err := NewListener(addr)\\n    if err != nil {\\n        log.DefaultLogger.Errorf(\\"create a listener failed: %v\\", err)\\n        return nil, err\\n    }\\n    // \u8c03\u7528 NewRuntimeGrpcServer\\n    srv, err := s.f(conf, options...)\\n    if err != nil {\\n        log.DefaultLogger.Errorf(\\"create a registered server failed: %v\\", err)\\n        return nil, err\\n    }\\n    sw = &registerServerWrapper{\\n        server: srv,\\n        ln:     ln,\\n    }\\n    s.servers[addr] = sw\\n    return sw, nil\\n}\\n=\\nmain.NewRunvtimeGrpcServer at main.go\\n=>\\nmosn.io/layotto/pkg/runtime.(*MosnRuntime).initRuntime at runtime.go\\n=>\\nmosn.io/layotto/pkg/runtime.(*MosnRuntime).initRpcs at runtime.go\\n=>\\nmosn.io/layotto/components/rpc/invoker/mosn.(*mosnInvoker).Init at mosninvoker.go\\nfunc (m *mosnInvoker) Init(conf rpc.RpcConfig) error {\\n    var config mosnConfig\\n    if err := json.Unmarshal(conf.Config, &config); err != nil {\\n        return err\\n    }\\n\\n    // \u521d\u59cb\u5316 RPC \u8c03\u7528\u524d\u7684 Filter\\n    for _, before := range config.Before {\\n        m.cb.AddBeforeInvoke(before)\\n    }\\n\\n    // \u521d\u59cb\u5316 RPC \u8c03\u7528\u540e\u7684 Filter\\n    for _, after := range config.After {\\n        m.cb.AddAfterInvoke(after)\\n    }\\n\\n    if len(config.Channel) == 0 {\\n        return errors.New(\\"missing channel config\\")\\n    }\\n\\n    // \u521d\u59cb\u5316\u4e0e Mosn \u901a\u4fe1\u4f7f\u7528\u7684\u901a\u9053\u3001\u534f\u8bae\u53ca\u5bf9\u5e94\u7aef\u53e3\\n    channel, err := channel.GetChannel(config.Channel[0])\\n    if err != nil {\\n        return err\\n    }\\n    m.channel = channel\\n    return nil\\n}\\n...\\n// \u5b8c\u6210\u4e00\u4e9b\u5217\u521d\u59cb\u5316\u540e\u5728 grpcServerFilter \u4e2d\u542f\u52a8 Grpc Server\\nmosn.io/mosn/pkg/filter/network/grpc.(*grpcServerFilterFactory).Init at factory.go\\nfunc (f *grpcServerFilterFactory) Init(param interface{}) error {\\n    ...\\n    opts := []grpc.ServerOption{\\n        grpc.UnaryInterceptor(f.UnaryInterceptorFilter),\\n        grpc.StreamInterceptor(f.StreamInterceptorFilter),\\n    }\\n    // \u7ecf\u8fc7\u4e0a\u8ff0\u521d\u59cb\u5316\uff0c\u5b8c\u6210 Grpc registerServerWrapper \u7684\u521d\u59cb\u5316\\n    sw, err := f.handler.New(addr, f.config.GrpcConfig, opts...)\\n    if err != nil {\\n        return err\\n    }\\n    // \u542f\u52a8 Grpc sever\\n    sw.Start(f.config.GracefulStopTimeout)\\n    f.server = sw\\n    log.DefaultLogger.Debugf(\\"grpc server filter initialized success\\")\\n    return nil\\n}\\n...\\n// StageManager \u5728 runInitStage \u4e4b\u540e\u8fdb\u5165 runStartStage \u542f\u52a8 Mosn\\nfunc (stm *StageManager) runStartStage() {\\n    st := time.Now()\\n    stm.SetState(Starting)\\n    for _, f := range stm.startupStages {\\n        f(stm.app)\\n    }\\n\\n    stm.wg.Add(1)\\n    // \u5728\u6240\u6709\u542f\u52a8\u9636\u6bb5\u5b8c\u6210\u540e\u542f\u52a8 Mosn\\n    stm.app.Start()\\n    ...\\n}\\n```\\n\\n### 0x01 Dubbo-go-sample client request\\n\\nFollow the example of [Dubbo Json Rpc Example](https://mosn.io/layotto/#/en/start/rpc/dub_json_rpc)\\n\\n```shell\\ngo un demo/rpc/dubbo_json_rpc/dub_json_client/client.go -d \'{\\"jsonrpc\\": \\"2.0\\", \\"method\\":\\"GetUser\\", \\"params\\":[\\"A003\\"],\\"id\\":9527}\'\\n```\\n\\nUse Layotto for App Gypc API InvokeService initiate RPC calls, data filling and connecting processes leading to the dispatch of data to Layotto via SendMsg in Grpc clientStream, as follows.\\n\\n```go\\n\\nfunc main() {\\n    data := flag.String(\\"d\\", `{\\"jsonrpc\\":\\"2.0\\",\\"method\\":\\"GetUser\\",\\"params\\":[\\"A003\\"],\\"id\\":9527}`, \\"-d\\")\\n    flag.Parse()\\n    \\n    conn, err := grpc.Dial(\\"localhost:34904\\", grpc.WithInsecure())\\n    if err != nil {\\n        log.Fatal(err)\\n    }\\n\\n    cli := runtimev1pb.NewRuntimeClient(conn)\\n    ctx, cancel := context.WithCancel(context.TODO())\\n    defer cancel()\\n    // \u901a\u8fc7 Grpc \u63a5\u53e3 InvokeService \u8fdb\u884c RPC \u8c03\u7528\\n    resp, err := cli.InvokeService(\\n        ctx,\\n       // \u4f7f\u7528 runtimev1pb.InvokeServiceRequest \u53d1\u8d77 Grpc \u8bf7\u6c42\\n        &runtimev1pb.InvokeServiceRequest{\\n           // \u8981\u8bf7\u6c42\u7684 server \u63a5\u53e3 ID\\n           Id: \\"org.apache.dubbo.samples.UserProvider\\",\\n            Message: &runtimev1pb.CommonInvokeRequest{\\n               // \u8981\u8bf7\u6c42\u7684\u63a5\u53e3\u5bf9\u5e94\u7684\u65b9\u6cd5\u540d\\n                Method:        \\"GetUser\\",\\n                ContentType:   \\"\\",\\n                Data:          &anypb.Any{Value: []byte(*data)},\\n                HttpExtension: &runtimev1pb.HTTPExtension{Verb: runtimev1pb.HTTPExtension_POST},\\n            },\\n        },\\n    )\\n    if err != nil {\\n        log.Fatal(err)\\n    }\\n\\n    fmt.Println(string(resp.Data.GetValue()))\\n}\\n=>\\nmosn.io/layotto/spec/proto/runtime/v1.(*runtimeClient).InvokeService at runtime.pb.go\\n=>\\ngoogle.golang.org/grpc.(*ClientConn).Invoke at call.go\\n=>\\ngoogle.golang.org/grpc.(*clientStream).SendMsg at stream.go\\n=>\\ngoogle.golang.org/grpc.(*csAttempt).sendMsg at stream.go\\n=>\\ngoogle.golang.org/grpc/internal/transport.(*http2Client).Write at http2_client.go\\n```\\n\\n### 0x02 Mosn EventLoop Reader Processing Request Data\\n\\nThe kernel from Layotto mentioned above is a mock-up of Mosn, so when network connection data arrives, it will first be read and written at the L4 network level in Mosn as follows.\\n\\n```go\\nmosn.io/mosn/pkg/network.(*listener).accept at listener.go\\n=>\\nmosn.io/mosn/pkg/server.(*activeListener).OnAccept at handler.go\\n=>\\nmosn.io/mosn/pkg/server.(*activeRawConn).ContinueFilterChain at handler.go\\n=>\\nmosn.io/mosn/pkg/server.(*activeListener).newConnection at handler.go\\n=>\\nmosn.io/mosn/pkg/network.(*connection).Start at connection.go\\n=>\\nmosn.io/mosn/pkg/network.(*connection).startRWLoop at connection.go\\nfunc (c *connection) startRWLoop(lctx context.Context) {\\n    c.internalLoopStarted = true\\n\\n    utils.GoWithRecover(func() {\\n       // \u8bfb\u534f\u7a0b\\n        c.startReadLoop()\\n    }, func(r interface{}) {\\n        c.Close(api.NoFlush, api.LocalClose)\\n    })\\n\\n    if c.checkUseWriteLoop() {\\n        c.useWriteLoop = true\\n        utils.GoWithRecover(func() {\\n           // \u5199\u534f\u7a0b\\n            c.startWriteLoop()\\n        }, func(r interface{}) {\\n            c.Close(api.NoFlush, api.LocalClose)\\n        })\\n    }\\n}\\n```\\n\\nIn the startRWLoop method, we can see that two separate protocols will be opened to deal with reading and writing operations on the connection: startReadLoop and startWriteLoop; the following streams will be made in startReadLoop; the data read at the network level will be handled by the filterManager filter chain, as follows.\\n\\n```go\\nmosn.io/mosn/pkg/network.(*connection).doRead at connection.go\\n=>\\nmosn.io/mosn/pkg/network.(*connection).onRead at connection.go\\n=>\\nmosn.io/mosn/pkg/network.(*filterManager).OnRead at filtermanager.go\\n=>\\nmosn.io/mosn/pkg/network.(*filterManager).onContinueReading at filtermanager.go\\nfunc (fm *filterManager) onContinueReading(filter *activeReadFilter) {\\n    var index int\\n    var uf *activeReadFilter\\n\\n    if filter != nil {\\n        index = filter.index + 1\\n    }\\n\\n    // filterManager\u904d\u5386\u8fc7\u6ee4\u5668\u8fdb\u884c\u6570\u636e\u5904\u7406\\n    for ; index < len(fm.upstreamFilters); index++ {\\n        uf = fm.upstreamFilters[index]\\n        uf.index = index\\n        // \u5bf9\u6ca1\u6709\u521d\u59cb\u5316\u7684\u8fc7\u6ee4\u5668\u8c03\u7528\u5176\u521d\u59cb\u5316\u65b9\u6cd5 OnNewConnection\uff0c\u672c\u4f8b\u4e3afunc (f *grpcFilter) OnNewConnection() api.FilterStatus\uff08\u5411 Listener \u53d1\u9001 grpc \u8fde\u63a5\u4ee5\u5524\u9192 Listener \u7684 Accept\uff09\\n        if !uf.initialized {\\n            uf.initialized = true\\n\\n            status := uf.filter.OnNewConnection()\\n\\n            if status == api.Stop {\\n                return\\n            }\\n        }\\n\\n        buf := fm.conn.GetReadBuffer()\\n\\n        if buf != nil && buf.Len() > 0 {\\n           // \u901a\u77e5\u76f8\u5e94\u8fc7\u6ee4\u5668\u5904\u7406\\n            status := uf.filter.OnData(buf)\\n\\n            if status == api.Stop {\\n                return\\n            }\\n        }\\n    }\\n}\\n=>\\nmosn.io/mosn/pkg/filter/network/grpc.(*grpcFilter).OnData at filter.go\\n=>\\nmosn.io/mosn/pkg/filter/network/grpc.(*grpcFilter).dispatch at filter.go\\nfunc (f *grpcFilter) dispatch(buf buffer.IoBuffer) {\\n    if log.DefaultLogger.GetLogLevel() >= log.DEBUG {\\n        log.DefaultLogger.Debugf(\\"grpc get datas: %d\\", buf.Len())\\n    }\\n    // \u53d1\u9001\u6570\u636e\u5524\u9192\u8fde\u63a5\u8bfb\u53d6\\n    f.conn.Send(buf)\\n    if log.DefaultLogger.GetLogLevel() >= log.DEBUG {\\n        log.DefaultLogger.Debugf(\\"read dispatch finished\\")\\n    }\\n}\\n```\\n\\n### 0x03 Grpc Sever processed requests as NetworkFilter\\n\\nReading data from the original connection in the first phase will enter the Grpc Serve handling, the Serve method will use the net.Listener listener, each time a new protocol is launched to handle the new connection (handleRawCon), and a RPC call based on Http2-based transport will be set out below.\\n\\n```go\\ngoogle.golang.org/grpc.(*Server).handleRawConn at server.go\\nfunc (s *Server) handleRawConn(lisAddr string, rawConn net.Conn) {\\n    // \u6821\u9a8c\u670d\u52a1\u72b6\u6001\\n    if s.quit.HasFired() {\\n        rawConn.Close()\\n        return\\n    }\\n    rawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout))\\n    conn, authInfo, err := s.useTransportAuthenticator(rawConn)\\n    if err != nil {\\n       ...\\n    }\\n    // HTTP2 \u63e1\u624b\uff0c\u521b\u5efa Http2Server \u4e0e\u5ba2\u6237\u7aef\u4ea4\u6362\u5e27\u7684\u521d\u59cb\u5316\u4fe1\u606f\uff0c\u5e27\u548c\u7a97\u53e3\u5927\u5c0f\u7b49\\n    st := s.newHTTP2Transport(conn, authInfo)\\n    if st == nil {\\n        return\\n    }\\n\\n    rawConn.SetDeadline(time.Time{})\\n    if !s.addConn(lisAddr, st) {\\n        return\\n    }\\n    // \u521b\u5efa\u4e00\u4e2a\u534f\u7a0b\u8fdb\u884c\u6d41\u5904\u7406\\n    go func() {\\n        s.serveStreams(st)\\n        s.removeConn(lisAddr, st)\\n    }()\\n    ...\\n}\\n=>\\ngoogle.golang.org/grpc.(*Server).serveStreams at server.go\\n=>\\ngoogle.golang.org/grpc.(*Server).handleStream at server.go\\nfunc (s *Server) handleStream(t transport.ServerTransport, stream *transport.Stream, trInfo *traceInfo) {\\n    // \u627e\u5230\u5230\u9700\u8981\u8c03\u7528\u7684 FullMethod\uff0c\u6b64\u4f8b\u4e3a spec.proto.runtime.v1.Runtime/InvokeService\\n    sm := stream.Method()\\n    if sm != \\"\\" && sm[0] == \'/\' {\\n        sm = sm[1:]\\n    }\\n    ...\\n    service := sm[:pos]\\n    method := sm[pos+1:]\\n\\n    // \u4ece\u6ce8\u518c\u7684 service \u5217\u8868\u4e2d\u627e\u5230\u5bf9\u5e94 serviceInfo \u5bf9\u8c61\\n    srv, knownService := s.services[service]\\n    if knownService {\\n        // \u6839\u636e\u65b9\u6cd5\u540d\u627e\u5230\u5355\u5411\u8bf7\u6c42\u7684 md\u2014\u2014MethodDesc\uff0c\u6b64 demo \u4e3a mosn.io/layotto/spec/proto/runtime/v1._Runtime_InvokeService_Handler\\n        if md, ok := srv.methods[method]; ok {\\n            s.processUnaryRPC(t, stream, srv, md, trInfo)\\n            return\\n        }\\n        // \u6d41\u5f0f\u8bf7\u6c42\\n        if sd, ok := srv.streams[method]; ok {\\n            s.processStreamingRPC(t, stream, srv, sd, trInfo)\\n            return\\n        }\\n    }\\n    ...\\n=>\\ngoogle.golang.org/grpc.(*Server).processUnaryRPC at server.go\\n=>\\nmosn.io/layotto/spec/proto/runtime/v1._Runtime_InvokeService_Handler at runtime.pb.go\\n=>\\ngoogle.golang.org/grpc.chainUnaryServerInterceptors at server.go\\n=>\\n// \u670d\u52a1\u7aef\u5355\u5411\u8c03\u7528\u62e6\u622a\u5668\uff0c\u7528\u4ee5\u8c03\u7528 Mosn \u7684 streamfilter\\nmosn.io/mosn/pkg/filter/network/grpc.(*grpcServerFilterFactory).UnaryInterceptorFilter at factory.go\\n=>\\ngoogle.golang.org/grpc.getChainUnaryHandler at server.go\\n// \u9012\u5f52\u751f\u6210\u94fe\u5f0fUnaryHandler\\nfunc getChainUnaryHandler(interceptors []UnaryServerInterceptor, curr int, info *UnaryServerInfo, finalHandler UnaryHandler) UnaryHandler {\\n    if curr == len(interceptors)-1 {\\n        return finalHandler\\n    }\\n\\n    return func(ctx context.Context, req interface{}) (interface{}, error) {\\n       // finalHandler\u5c31\u662fmosn.io/layotto/spec/proto/runtime/v1._Runtime_InvokeService_Handler\\n        return interceptors[curr+1](ctx, req, info, getChainUnaryHandler(interceptors, curr+1, info, finalHandler))\\n    }\\n}\\n```\\n\\n### 0x04 Layotto send RPC requests and write to local virtual connections\\n\\nThe 0x03 process follows Runtime_InvokeService_Handler, converted from the GRPC Default API to Dapr API, entering the light RPC framework provided by Layotto in Mosn, as follows.\\n\\n```go\\nmosn.io/layotto/spec/proto/runtime/v1._Runtime_InvokeService_Handler at runtime.pb.go\\n=>\\nmosn.io/layotto/pkg/grpc/default_api.(*api).InvokeService at api.go\\n=>\\nmosn.io/layotto/pkg/grpc/dapr.(*daprGrpcAPI).InvokeService at dapr_api.go\\n=>\\nmosn.io/layotto/components/rpc/invoker/mosn.(*mosnInvoker).Invoke at mosninvoker.go\\n// \u8bf7\u6c42 Mosn \u5e95\u5ea7\u548c\u8fd4\u56de\u54cd\u5e94\\nfunc (m *mosnInvoker) Invoke(ctx context.Context, req *rpc.RPCRequest) (resp *rpc.RPCResponse, err error) {\\n    defer func() {\\n        if r := recover(); r != nil {\\n            err = fmt.Errorf(\\"[runtime][rpc]mosn invoker panic: %v\\", r)\\n            log.DefaultLogger.Errorf(\\"%v\\", err)\\n        }\\n    }()\\n\\n    // 1. \u5982\u679c\u8d85\u65f6\u65f6\u95f4\u4e3a 0\uff0c\u8bbe\u7f6e\u9ed8\u8ba4 3000ms \u8d85\u65f6\\n    if req.Timeout == 0 {\\n        req.Timeout = 3000\\n    }\\n    req.Ctx = ctx\\n    log.DefaultLogger.Debugf(\\"[runtime][rpc]request %+v\\", req)\\n    // 2. \u89e6\u53d1\u8bf7\u6c42\u6267\u884c\u524d\u7684\u81ea\u5b9a\u4e49\u903b\u8f91\\n    req, err = m.cb.BeforeInvoke(req)\\n    if err != nil {\\n        log.DefaultLogger.Errorf(\\"[runtime][rpc]before filter error %s\\", err.Error())\\n        return nil, err\\n    }\\n    // 3. \u6838\u5fc3\u8c03\u7528\uff0c\u4e0b\u6587\u4f1a\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\\n    resp, err = m.channel.Do(req)\\n    if err != nil {\\n        log.DefaultLogger.Errorf(\\"[runtime][rpc]error %s\\", err.Error())\\n        return nil, err\\n    }\\n    resp.Ctx = req.Ctx\\n    // 4. \u89e6\u53d1\u8bf7\u6c42\u8fd4\u56de\u540e\u7684\u81ea\u5b9a\u4e49\u903b\u8f91\\n    resp, err = m.cb.AfterInvoke(resp)\\n    if err != nil {\\n        log.DefaultLogger.Errorf(\\"[runtime][rpc]after filter error %s\\", err.Error())\\n    }\\n    return resp, err\\n}\\n=>\\nmosn.io/layotto/components/rpc/invoker/mosn/channel.(*httpChannel).Do at httpchannel.go\\nfunc (h *httpChannel) Do(req *rpc.RPCRequest) (*rpc.RPCResponse, error) {\\n    // 1. \u4f7f\u7528\u4e0a\u4e00\u9636\u6bb5\u8bbe\u7f6e\u7684\u9ed8\u8ba4\u8d85\u65f6\u8bbe\u7f6e context \u8d85\u65f6\\n    timeout := time.Duration(req.Timeout) * time.Millisecond\\n    ctx, cancel := context.WithTimeout(req.Ctx, timeout)\\n    defer cancel()\\n\\n    // 2. \u521b\u5efa\u8fde\u63a5\u5f97\u5230\uff0c\u542f\u52a8 readloop \u534f\u7a0b\u8fdb\u884c Layotto \u548c Mosn \u7684\u8bfb\u5199\u4ea4\u4e92\uff08\u5177\u4f53\u89c1\u4e0b\u6587\u5206\u6790\uff09\\n    conn, err := h.pool.Get(ctx)\\n    if err != nil {\\n        return nil, err\\n    }\\n    \\n    // 3. \u8bbe\u7f6e\u6570\u636e\u5199\u5165\u8fde\u63a5\u7684\u8d85\u65f6\u65f6\u95f4\\n    hstate := conn.state.(*hstate)\\n    deadline, _ := ctx.Deadline()\\n    if err = conn.SetWriteDeadline(deadline); err != nil {\\n        hstate.close()\\n        h.pool.Put(conn, true)\\n        return nil, common.Error(common.UnavailebleCode, err.Error())\\n    }\\n    // 4. \u56e0\u4e3a\u521d\u59cb\u5316\u65f6\u914d\u7f6e\u7684 Layotto \u4e0e Mosn \u4ea4\u4e92\u4f7f\u7528\u7684\u662f Http \u534f\u8bae\uff0c\u6240\u4ee5\u8fd9\u91cc\u4f1a\u6784\u9020 Http \u8bf7\u6c42\\n    httpReq := h.constructReq(req)\\n    defer fasthttp.ReleaseRequest(httpReq)\\n\\n    // \u501f\u52a9 fasthttp \u8bf7\u6c42\u4f53\u5199\u5165\u865a\u62df\u8fde\u63a5\\n    if _, err = httpReq.WriteTo(conn); err != nil {\\n        hstate.close()\\n        h.pool.Put(conn, true)\\n        return nil, common.Error(common.UnavailebleCode, err.Error())\\n    }\\n\\n    // 5. \u6784\u9020 fasthttp.Response \u7ed3\u6784\u4f53\u8bfb\u53d6\u548c\u89e3\u6790 hstate \u7684\u8fd4\u56de\uff0c\u5e76\u8bbe\u7f6e\u8bfb\u53d6\u8d85\u65f6\u65f6\u95f4\\n    httpResp := &fasthttp.Response{}\\n    hstate.reader.SetReadDeadline(deadline)\\n\\n    // \u5728 Mosn \u6570\u636e\u8fd4\u56de\u524d\u8fd9\u91cc\u4f1a\u963b\u585e\uff0creadloop \u534f\u7a0b\u8bfb\u53d6 Mosn \u8fd4\u56de\u7684\u6570\u636e\u4e4b\u540e\u6d41\u7a0b\u89c1\u4e0b\u8ff0 0x08 \u9636\u6bb5\\n    if err = httpResp.Read(bufio.NewReader(hstate.reader)); err != nil {\\n        hstate.close()\\n        h.pool.Put(conn, true)\\n        return nil, common.Error(common.UnavailebleCode, err.Error())\\n    }\\n    h.pool.Put(conn, false)\\n    ...\\n}\\n=>\\nmosn.io/layotto/components/rpc/invoker/mosn/channel.(*connPool).Get at connpool.go\\n// Get is get wrapConn by context.Context\\nfunc (p *connPool) Get(ctx context.Context) (*wrapConn, error) {\\n    if err := p.waitTurn(ctx); err != nil {\\n        return nil, err\\n    }\\n\\n    p.mu.Lock()\\n    // 1. \u4ece\u8fde\u63a5\u6c60\u83b7\u53d6\u8fde\u63a5\\n    if ele := p.free.Front(); ele != nil {\\n        p.free.Remove(ele)\\n        p.mu.Unlock()\\n        wc := ele.Value.(*wrapConn)\\n        if !wc.isClose() {\\n            return wc, nil\\n        }\\n    } else {\\n        p.mu.Unlock()\\n    }\\n\\n    // 2. \u521b\u5efa\u65b0\u7684\u8fde\u63a5\\n    c, err := p.dialFunc()\\n    if err != nil {\\n        p.freeTurn()\\n        return nil, err\\n    }\\n    wc := &wrapConn{Conn: c}\\n    if p.stateFunc != nil {\\n        wc.state = p.stateFunc()\\n    }\\n    // 3. \u542f\u52a8 readloop \u72ec\u7acb\u534f\u7a0b\u8bfb\u53d6 Mosn \u8fd4\u56de\u7684\u6570\u636e\\n    if p.onDataFunc != nil {\\n        utils.GoWithRecover(func() {\\n            p.readloop(wc)\\n        }, nil)\\n    }\\n    return wc, nil\\n}\\n=>\\n```\\n\\nThe creation of a new connection in the second step above requires attention by calling dialFunc func() in the protocol that initialized the init phase (net.Conn, error), because the configuration interacted with Mosn with Http protocols, this is newHttpChanel, which is currently supported by the Bolt, Dubbo et al.\\n\\n```go\\nmosn.io/layotto/components/rpc/invoker/mosn/channel.newHttpChannel at httpchannel.go\\n// newHttpChannel is used to create rpc.Channel according to ChannelConfig\\nfunc newHttpChannel(config ChannelConfig) (rpc.Channel, error) {\\n    hc := &httpChannel{}\\n    // \u4e3a\u51cf\u5c11\u8fde\u63a5\u521b\u5efa\u5f00\u9500\u7684\u8fde\u63a5\u6c60\uff0c\u5b9a\u4e49\u5728 mosn.io/layotto/components/rpc/invoker/mosn/channel/connpool.go\\n    hc.pool = newConnPool(\\n        config.Size,\\n        // dialFunc\\n        func() (net.Conn, error) {\\n            _, _, err := net.SplitHostPort(config.Listener)\\n            if err == nil {\\n                return net.Dial(\\"tcp\\", config.Listener)\\n            }\\n           //\u521b\u5efa\u4e00\u5bf9\u865a\u62df\u8fde\u63a5(net.Pipe)\uff0cLayotto \u6301\u6709 local\uff0cMosn \u6301\u6709 remote, Layotto \u5411 local \u5199\u5165\uff0cMosn \u4f1a\u6536\u5230\u6570\u636e, Mosn \u4ece remote\u8bfb\u53d6\uff0c\u6267\u884c filter \u903b\u8f91\u5e76\u8fdb\u884c\u4ee3\u7406\u8f6c\u53d1\uff0c\u518d\u5c06\u54cd\u5e94\u5199\u5230 remote ,\u6700\u540e Layotto \u4ece remote \u8bfb\u53d6\uff0c\u83b7\u5f97\u54cd\u5e94\\n            local, remote := net.Pipe()\\n            localTcpConn := &fakeTcpConn{c: local}\\n            remoteTcpConn := &fakeTcpConn{c: remote}\\n           // acceptFunc \u662f\u5b9a\u4e49\u5728 mosn.io/layotto/components/rpc/invoker/mosn/channel.go \u4e2d\u7684\u95ed\u5305\uff0c\u95ed\u5305\u4e2d\u76d1\u542c\u4e86 remote \u865a\u62df\u8fde\u63a5\\n            if err := acceptFunc(remoteTcpConn, config.Listener); err != nil {\\n                return nil, err\\n            }\\n            // the goroutine model is:\\n            // request goroutine ---\x3e  localTcpConn ---\x3e mosn\\n            //        ^                                        |\\n            //        |                                        |\\n            //        |                                        |\\n            //         hstate <-- readloop goroutine     <------\\n            return localTcpConn, nil\\n        },\\n        // stateFunc\\n        func() interface{} {\\n            // hstate \u662f readloop \u534f\u7a0b\u4e0e request \u534f\u7a0b\u901a\u4fe1\u7684\u7ba1\u9053\uff0c\u662f\u4e00\u5bf9\u8bfb\u5199 net.Conn\uff0c\u8bf7\u6c42\u534f\u7a0b\u4ece reader net.Conn \u4e2d\u8bfb\u6570\u636e\uff0creadloop \u534f\u7a0b\u5e8f\u5f80 writer net.Conn \u5199\u6570\u636e\\n            s := &hstate{}\\n            s.reader, s.writer = net.Pipe()\\n            return s\\n        },\\n        hc.onData,\\n        hc.cleanup,\\n    )\\n    return hc, nil\\n}\\n```\\n\\n### 0x05 Mosn read Remote and execute Filter and proxy forwarding\\n\\n(1) Similar to 0x02, filtermanager executes the filter processing phase where proxy forwarding is made in proxy with the following code.\\n\\n```go\\n...\\nmosn.io/mosn/pkg/network.(*filterManager).onContinueReading at filtermanager.go\\n=>\\nmosn.io/mosn/pkg/proxy.(*proxy).OnData at proxy.go\\nfunc (p *proxy) OnData(buf buffer.IoBuffer) api.FilterStatus {\\n    if p.fallback {\\n        return api.Continue\\n    }\\n\\n    if p.serverStreamConn == nil {\\n        ...\\n        p.serverStreamConn = stream.CreateServerStreamConnection(p.context, proto, p.readCallbacks.Connection(), p)\\n    }\\n    //\u628a\u6570\u636e\u5206\u53d1\u5230\u5bf9\u5e94\u534f\u8bae\u7684\u89e3\u7801\u5668\uff0c\u5728\u8fd9\u91cc\u56e0\u4e3a\u662f POST /org.apache.dubbo.samples.UserProvider HTTP/1.1\uff0c\u6240\u4ee5\u662f mosn.io/mosn/pkg/stream/http.(*serverStreamConnection).serve at stream.go\\n    p.serverStreamConn.Dispatch(buf)\\n\\n    return api.Stop\\n}\\n=>\\n```\\n\\n(2) ServerStreamConnection.serve listens and handles requests to downstream OnReceive, as described below.\\n\\n```go\\nmosn.io/mosn/pkg/stream/http.(*serverStream).handleRequest at stream.go\\nfunc (s *serverStream) handleRequest(ctx context.Context) {\\n    if s.request != nil {\\n        // set non-header info in request-line, like method, uri\\n        injectCtxVarFromProtocolHeaders(ctx, s.header, s.request.URI())\\n        hasData := true\\n        if len(s.request.Body()) == 0 {\\n            hasData = false\\n        }\\n\\n        if hasData {\\n           //\u5728\u6b64\u8fdb\u5165 downstream OnReceive\\n            s.receiver.OnReceive(s.ctx, s.header, buffer.NewIoBufferBytes(s.request.Body()), nil)\\n        } else {\\n            s.receiver.OnReceive(s.ctx, s.header, nil, nil)\\n        }\\n    }\\n}\\n=>\\nmosn.io/mosn/pkg/proxy.(*downStream).OnReceive at downstream.go\\nfunc (s *downStream) OnReceive(ctx context.Context, headers types.HeaderMap, data types.IoBuffer, trailers types.HeaderMap) {\\n    ...\\n    var task = func() {\\n        ...\\n\\n        phase := types.InitPhase\\n        for i := 0; i < 10; i++ {\\n            s.cleanNotify()\\n\\n            phase = s.receive(s.context, id, phase)\\n            ...\\n            }\\n        }\\n    }\\n\\n    if s.proxy.serverStreamConn.EnableWorkerPool() {\\n        if s.proxy.workerpool != nil {\\n            // use the worker pool for current proxy\\n            s.proxy.workerpool.Schedule(task)\\n        } else {\\n            // use the global shared worker pool\\n            pool.ScheduleAuto(task)\\n        }\\n        return\\n    }\\n\\n    task()\\n    return\\n\\n}\\n```\\n\\n(3) The above ScheduleAuto schedule, after processing the reveive of downstream Stream, processing upstam Request, as well as an application with an application from the network layer, eventually sending data from connection.Write and entering WaitNotify phases, as detailed below.\\n\\n```go\\nmosn.io/mosn/pkg/sync.(*workerPool).ScheduleAuto at workerpool.go\\n=>\\nmosn.io/mosn/pkg/sync.(*workerPool).spawnWorker at workerpool.go\\n=>\\nmosn.io/mosn/pkg/proxy.(*downStream).receive at downstream.go\\n=>\\nInitPhase=>DownFilter=>MatchRoute=>DownFilterAfterRoute=>ChooseHost=>DownFilterAfterChooseHost=>DownRecvHeader=>DownRecvData\\n=>\\nmosn.io/mosn/pkg/proxy.(*downStream).receiveData at downstream.go\\n=>\\nmosn.io/mosn/pkg/proxy.(*upstreamRequest).appendData at upstream.go\\n=>\\nmosn.io/mosn/pkg/stream/http.(*clientStream).doSend at stream.go\\n=>\\ngithub.com/valyala/fasthttp.(*Request).WriteTo at http.go\\n=>\\nmosn.io/mosn/pkg/stream/http.(*streamConnection).Write at stream.go\\n>\\nmosn.io/mosn/pkg/network.(*connection).Write at connection.go\\n=>\\nmosn.io/mosn/pkg/proxy.(*downStream).receive at downstream.go\\nfunc (s *downStream) receive(ctx context.Context, id uint32, phase types.Phase) types.Phase {\\n    for i := 0; i <= int(types.End-types.InitPhase); i++ {\\n        s.phase = phase\\n        \\n        switch phase {\\n        ...\\n        case types.WaitNotify:\\n            s.printPhaseInfo(phase, id)\\n            if p, err := s.waitNotify(id); err != nil {\\n                return p\\n            }\\n        \\n            if log.Proxy.GetLogLevel() >= log.DEBUG {\\n            \\tlog.Proxy.Debugf(s.context, \\"[proxy] [downstream] OnReceive send downstream response %+v\\", s.downstreamRespHeaders)\\n            }\\n        ...\\n} \\n=>\\nfunc (s *downStream) waitNotify(id uint32) (phase types.Phase, err error) {\\n    if atomic.LoadUint32(&s.ID) != id {\\n        return types.End, types.ErrExit\\n    }\\n\\n\\tif log.Proxy.GetLogLevel() >= log.DEBUG {\\n\\t\\tlog.Proxy.Debugf(s.context, \\"[proxy] [downstream] waitNotify begin %p, proxyId = %d\\", s, s.ID)\\n\\t}\\n\\tselect {\\n\\t// \u963b\u585e\u7b49\u5f85\\n\\tcase <-s.notify:\\n\\t}\\n\\treturn s.processError(id)\\n}\\n```\\n\\n### 0x06 Dubbo-go-sample server received request return response\\n\\nHere is a dubo-go-sample server handling, leave it now, post log messages and check the source code by interested classes.\\n\\n```\\n[2022-04-18/21:03:03:18 github.com/apache/dub-go-samples/rpc/jsonrpc/go-server/pkg.(*UserProvider2).GetUser: user_provider2.go: 53] userID: \\"A003\\"\\n[2022-04-18/21:03:18 github.com/apache/dub-go-samples/rpc/jsonrpc/go-server/pkg. (*UserProvider2).GetUser: user_provider2.go: 56] rsp:&pkg.User{ID:\\"113\\", Name:\\"Moorse\\", Age:30, sex:0, Birth:703391943, Sex:\\"MAN\\"MAN\\"}\\n```\\n\\n### 0x07 Mosn framework handles responses and writes back to Remote Virtual Connection\\n\\nAfter the third phase of 0x05 above, the response logic goes into the UpRecvData phase of the reveive cycle phase through a series of final response writing back to the remote virtual connection at 0x04, as follows.\\n\\n```go\\nmosn.io/mosn/pkg/proxy.(*downStream).receive at downstream.go\\nfunc (s *downStream) waitNotify(id uint32) (phase types.Phase, err error) {\\n    if atomic.LoadUint32(&s.ID) != id {\\n        return types.End, types.ErrExit\\n    }\\n    \\n    if log.Proxy.GetLogLevel() >= log.DEBUG {\\n        log.Proxy.Debugf(s.context, \\"[proxy] [downstream] waitNotify begin %p, proxyId = %d\\", s, s.ID)\\n    }\\n    // \u8fd4\u56de\u54cd\u5e94\\n    select {\\n    case <-s.notify:\\n    }\\n    return s.processError(id)\\n}\\n=>\\nUpFilter\\n=>\\nUpRecvHeader\\n=>\\nfunc (s *downStream) receive(ctx context.Context, id uint32, phase types.Phase) types.Phase {\\n    for i := 0; i <= int(types.End-types.InitPhase); i++ {\\n        s.phase = phase\\n\\n        switch phase {\\n        ...\\n        case types.UpRecvData:\\n            if s.downstreamRespDataBuf != nil {\\n            \\ts.printPhaseInfo(phase, id)\\n            \\ts.upstreamRequest.receiveData(s.downstreamRespTrailers == nil)\\n                if p, err := s.processError(id); err != nil {\\n              \\t   return p\\n              }\\n           }\\n        ...\\n}\\n=>\\nmosn.io/mosn/pkg/proxy.(*upstreamRequest).receiveData at upstream.go\\n=>\\nmosn.io/mosn/pkg/proxy.(*downStream).onUpstreamData at downstream.go\\n=>\\nmosn.io/mosn/pkg/proxy.(*downStream).appendData at downstream.go\\n=>\\nmosn.io/mosn/pkg/stream/http.(*serverStream).AppendData at stream.go\\n=>\\nmosn.io/mosn/pkg/stream/http.(*serverStream).endStream at stream.go\\n=>\\nmosn.io/mosn/pkg/stream/http.(*serverStream).doSend at stream.go\\n=>\\ngithub.com/valyala/fasthttp.(*Response).WriteTo at http.go\\n=>\\ngithub.com/valyala/fasthttp.writeBufio at http.go\\n=>\\ngithub.com/valyala/fasthttp.(*statsWriter).Write at http.go\\n=>\\nmosn.io/mosn/pkg/stream/http.(*streamConnection).Write at stream.go\\n```\\n\\n### 0x08 Layotto receive RPC responses and read Local Virtual Connection\\n\\nReadloop Reading IO, activated by 0x04 above, is activated from connection read data from Mosn and then forwarded to the hstate pipe to return to the request process, as follows.\\n\\n```go\\nmosn.io/layotto/components/rpc/invoker/mosn/channel.(*connPool).readloop at connpool.go\\n// readloop is loop to read connected then exec onDataFunc\\nfunc (p *connPool) readloop(c *wrapConn) {\\n    var err error\\n\\n    defer func() {\\n        c.close()\\n        if p.cleanupFunc != nil {\\n            p.cleanupFunc(c, err)\\n        }\\n    }()\\n\\n    c.buf = buffer.NewIoBuffer(defaultBufSize)\\n    for {\\n        // \u4ece\u8fde\u63a5\u8bfb\u53d6\u6570\u636e\\n        n, readErr := c.buf.ReadOnce(c)\\n        if readErr != nil {\\n            err = readErr\\n            if readErr == io.EOF {\\n                log.DefaultLogger.Debugf(\\"[runtime][rpc]connpool readloop err: %s\\", readErr.Error())\\n            } else {\\n                log.DefaultLogger.Errorf(\\"[runtime][rpc]connpool readloop err: %s\\", readErr.Error())\\n            }\\n        }\\n\\n        if n > 0 {\\n            // \u5728onDataFunc \u59d4\u6258\u7ed9 hstate \u5904\u7406\u6570\u636e\\n            if onDataErr := p.onDataFunc(c); onDataErr != nil {\\n                err = onDataErr\\n                log.DefaultLogger.Errorf(\\"[runtime][rpc]connpool onData err: %s\\", onDataErr.Error())\\n            }\\n        }\\n\\n        if err != nil {\\n            break\\n        }\\n\\n        if c.buf != nil && c.buf.Len() == 0 && c.buf.Cap() > maxBufSize {\\n            c.buf.Free()\\n            c.buf.Alloc(defaultBufSize)\\n        }\\n    }\\n}\\n=>\\nmosn.io/layotto/components/rpc/invoker/mosn/channel.(*httpChannel).onData at httpchannel.go\\n=>\\nmosn.io/layotto/components/rpc/invoker/mosn/channel.(*hstate).onData at httpchannel.go\\n=>\\nnet.(*pipe).Write at pipe.go\\n=>\\nmosn.io/layotto/components/rpc/invoker/mosn/channel.(*httpChannel).Do at httpchannel.go\\nfunc (h *httpChannel) Do(req *rpc.RPCRequest) (*rpc.RPCResponse, error) {\\n    ...\\n    // \u63a5\u4e0a\u8ff00x04\u9636\u6bb5\uff0cmosn \u6570\u636e\u8fd4\u56de\u540e\uff0c\u4ece hstate \u8bfb\u53d6 readloop \u534f\u7a0b\u4ece mosn \u8fd4\u56de\u7684\u6570\u636e\\n    if err = httpResp.Read(bufio.NewReader(hstate.reader)); err != nil {\\n        hstate.close()\\n        h.pool.Put(conn, true)\\n        return nil, common.Error(common.UnavailebleCode, err.Error())\\n    }\\n    h.pool.Put(conn, false)\\n\\n    // \u83b7\u53d6 fasthttp \u7684\u6570\u636e\u90e8\u5206\uff0c\u89e3\u6790\u72b6\u6001\u7801\uff0c\u5931\u8d25\u8fd4\u56de\u9519\u8bef\u4fe1\u606f\u548c\u72b6\u6001\u7801\\n    body := httpResp.Body()\\n    if httpResp.StatusCode() != http.StatusOK {\\n        return nil, common.Errorf(common.UnavailebleCode, \\"http response code %d, body: %s\\", httpResp.StatusCode(), string(body))\\n    }\\n    \\n    // 6. \u5c06\u7ed3\u679c\u8f6c\u6362\u4e3a rpc.RPCResponse \u8fd4\u56de\\n    rpcResp := &rpc.RPCResponse{\\n        ContentType: string(httpResp.Header.ContentType()),\\n        Data:        body,\\n        Header:      map[string][]string{},\\n    }\\n    httpResp.Header.VisitAll(func(key, value []byte) {\\n        rpcResp.Header[string(key)] = []string{string(value)}\\n    })\\n    return rpcResp, nil\\n```\\n\\n### 0x09 Grpc Sever processed data frames returned to clients\\n\\nGrpc does not write data directly to connections, but uses a systray loop to fetch frames from a cache structure and write them back to the client, as follows.\\n\\n```go\\ngoogle.golang.org/grpc/internal/transport.NewServerTransport at http2_server.go\\nfunc NewServerTransport(conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {\\n    ...\\n    // \u534f\u7a0b\u5f02\u6b65loop\u5faa\u73af\\n    go func() {\\n        t.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)\\n        t.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler\\n        if err := t.loopy.run(); err != nil {\\n            if logger.V(logLevel) {\\n                logger.Errorf(\\"transport: loopyWriter.run returning. Err: %v\\", err)\\n            }\\n        }\\n        t.conn.Close()\\n        t.controlBuf.finish()\\n        close(t.writerDone)\\n    }()\\n    go t.keepalive()\\n    return t, nil\\n}\\n=>\\ngoogle.golang.org/grpc/internal/transport.(*loopyWriter).run at controlbuf.go\\n=>\\ngoogle.golang.org/grpc/internal/transport.(*bufWriter).Flush at http_util.go\\n=>\\nmosn.io/mosn/pkg/filter/network/grpc.(*Connection).Write at conn.go\\n=>\\nmosn.io/mosn/pkg/network.(*connection).Write at connection.go\\n=>\\nmosn.io/mosn/pkg/network.(*connection).writeDirectly at connection.go\\n=>\\nmosn.io/mosn/pkg/network.(*connection).doWrite at connection.go\\n```\\n\\n### 0x10 dubbo-go-sample customer received response\\n\\nThe transmission of data from 0x01 above will be blocked in the client grpc bottom reading, and Layotto returns data from some of the processing layers above to enable ClientBottom Read IO, as follows.\\n\\n```go\\ngoogle.golang.org/grpc.(*ClientCon). Invoke at call.go\\n=>\\ngoogle.golang.org/grpc.(*ClientCon). Invoke at call.go\\n=>\\ngoogle.golang.org/grpc.(*clientStream). RecvMsg at stream. o\\n=>\\ngoogle.golang.org/grpc.(*clientStream).withRetry at stream.go\\n=>\\ngoogle.golang.org/grpc.(*csAtempt.recvMsg at stream.go\\n=>\\ngoogle.golang.org/grpc.recvAndDecompress at rpc_util. o\\n=>\\ngoogle.golang.org/grpc.recv at rpc_util.go\\n=>\\ngoogle.golang.org/grpc.(*parser).recvMsg at rpc_util.go\\n=>\\ngoogle.golang.org/grpc.(*csAttempt).recvMsg at stream. o\\nfunc (p *parser) recvMsg(maxReceiveMessageSize int) (pf payloadFormat, msg []byte, err error) LO\\n    if _, err := p. .Read(p.header[:]); err != nil {\\n        return 0, nil, err\\n    }\\n    ...\\n}\\n```\\n\\nLast returned data\uff1a\\n\\n```json\\n{\\"jsonrpc\\": \\"2.0\\", \\"id\\":9527, \\"result\\":{\\"id\\":\\"113\\", \\"name\\":\\"Moorse\\", \\"age\\":30,\\"time\\":703394193,\\"sex\\":\\"MAN\\"}}\\n```\\n\\n## Summary\\n\\nThe Layotto RPC process involves knowledge related to GRPC, Dapr, Mosn and others, and the overall process is lengthy, although it is clearer and simpler simply to see Layotto for Mosn an abstract lightweight RPC framework and is more innovative and useful for further study.Here Layotto RPC requests are analyzed and time-limited without some more comprehensive and in-depth profiles, such as defects, welcome contact\uff1arayo.wangzl@gmail.com.It is also hoped that there will be greater participation in source analysis and open source communities, learning together and making progress together."},{"id":"/code/start_process/start_process","metadata":{"permalink":"/en-US/blog/code/start_process/start_process","editUrl":"https://github.com/mosn/layotto/edit/main//i18n/en-US/docusaurus-plugin-content-blog/code/start_process/start_process.md","source":"@site/i18n/en-US/docusaurus-plugin-content-blog/code/start_process/start_process.md","title":"Source parsing layotto startup process","description":"Author Intro to\uff1a","date":"2024-07-05T07:00:37.000Z","tags":[],"readingTime":5.175,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Layotto Source Parsing \u2014 Processing RPC requests","permalink":"/en-US/blog/code/layotto-rpc"},"nextItem":{"title":"Layotto Source Parsing \u2014 WebAssembly","permalink":"/en-US/blog/code/webassembly"}},"content":"> Author Intro to\uff1a\\n> Libin, https://github.com/ZLBer\\n>\\n> Writing: 4 May 2022\\n\\n- [Overview](#Overview)\\n- [source analysis](#source analysis)\\n  - [1.cmd analysis](#1.cmd analysis)\\n  - [2.Callback functionNewRuntimeGrpcServer\u5206\u6790](#2.callback function NewRuntimeGrpcServer analysis)\\n  - [3.runtimeanalyze](#3.runtime analyse)\\n- [summary](#summary)\\n\\n## Overview\\n\\nLayotto \\"Parasite\\" in MOSN. The start process is in effect starting MOSN, MOSN back Layotto during startup to get Layotto start.\\n\\n## Source analysis\\n\\nEverything originating from our command line: layotto start -c `configpath`\\n\\n### 1.cmd analysis\\n\\nMain init function starts with\uff1a\\n\\n```\\nfunc init() {   \\n     //\u5c06layotto\u7684\u521d\u59cb\u5316\u51fd\u6570\u4f20\u7ed9mosn\uff0c\u8ba9mosn\u542f\u52a8\u7684\u65f6\u5019\u8fdb\u884c\u56de\u8c03\\n\\tmgrpc.RegisterServerHandler(\\"runtime\\", NewRuntimeGrpcServer)\\n     ....\\n}\\n```\\n\\ncmd action starts to execute\uff1a\\n\\n```\\n\\tAction: func(c *cli.Context) error {\\n\\t\\tapp := mosn.NewMosn()\\n\\t\\t//stagemanager\u7528\u4e8e\u7ba1\u7406mosn\u542f\u52a8\u7684\u6bcf\u4e2a\u9636\u6bb5\uff0c\u53ef\u4ee5\u6dfb\u52a0\u76f8\u5e94\u7684\u9636\u6bb5\u51fd\u6570\uff0c\u6bd4\u5982\u4e0b\u9762\u7684ParamsParsedStage\u3001InitStage\u3001PreStartStage\u3001AfterStartStage\\n\\t\\t//\u8fd9\u91cc\u662f\u5c06configpath\u4f20\u7ed9mosn\uff0c\u4e0b\u9762\u90fd\u662fmosn\u76f8\u5173\u7684\u903b\u8f91\\n\\t\\tstm := stagemanager.InitStageManager(c, c.String(\\"config\\"), app) \\n\\t\\tstm.AppendParamsParsedStage(ExtensionsRegister)\\n\\t\\tstm.AppendParamsParsedStage(func(c *cli.Context) {\\n\\t\\t\\terr := featuregate.Set(c.String(\\"feature-gates\\"))\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\tos.Exit(1)\\n\\t\\t\\t}\\n\\t\\t})\xb7\\n\\t\\tstm.AppendInitStage(mosn.DefaultInitStage)\\n\\t\\tstm.AppendPreStartStage(mosn.DefaultPreStartStage)\\n\\t\\tstm.AppendStartStage(mosn.DefaultStartStage)\\n\\t\\t//\u8fd9\u91cc\u6dfb\u52a0layotto\u7684\u5065\u5eb7\u68c0\u67e5\u673a\u5236\\n\\t\\tstm.AppendAfterStartStage(SetActuatorAfterStart)\\n\\t\\tstm.Run()\\n\\t\\t// wait mosn finished\\n\\t\\tstm.WaitFinish()\\n\\t\\treturn nil\\n\\t},\\n```\\n\\n### NewRuntimeGrpcServer Analysis\\n\\nReturns NewRuntimeGrpcServer when MOSN is launched, data is an unparsed configuration, opts is a grpc configuration, returning Gpc server\\n\\n```\\nfunc NewRuntimeGrpcServer(data json.RawMessage, opts ...grpc.ServerOption) (mgrpc.RegisteredServer, error) {\\n\\t// \u5c06\u539f\u59cb\u7684\u914d\u7f6e\u6587\u4ef6\u89e3\u6790\u6210\u7ed3\u6784\u4f53\u5f62\u5f0f\u3002\\n\\tcfg, err := runtime.ParseRuntimeConfig(data)\\n    // \u65b0\u5efalayotto runtime\uff0c runtime\u5305\u542b\u5404\u79cd\u7ec4\u4ef6\u7684\u6ce8\u518c\u5668\u548c\u5404\u79cd\u7ec4\u4ef6\u7684\u5b9e\u4f8b\u3002\\n\\trt := runtime.NewMosnRuntime(cfg)\\n\\t// 3.runtime\u5f00\u59cb\u542f\u52a8\\n\\tserver, err := rt.Run(\\n\\t       ...\\n        // 4. \u6dfb\u52a0\u6240\u6709\u7ec4\u4ef6\u7684\u521d\u59cb\u5316\u51fd\u6570\\n\\t \\t// \u6211\u4eec\u53ea\u770b\u4e0bFile\u7ec4\u4ef6\u7684\uff0c\u5c06NewXXX()\u6dfb\u52a0\u5230\u7ec4\u4ef6Factory\u91cc\\n\\t\\truntime.WithFileFactory(\\n\\t\\t\\tfile.NewFileFactory(\\"aliyun.oss\\", alicloud.NewAliCloudOSS),\\n\\t\\t\\tfile.NewFileFactory(\\"minio\\", minio.NewMinioOss),\\n\\t\\t\\tfile.NewFileFactory(\\"aws.s3\\", aws.NewAwsOss),\\n\\t\\t\\tfile.NewFileFactory(\\"tencent.oss\\", tencentcloud.NewTencentCloudOSS),\\n\\t\\t\\tfile.NewFileFactory(\\"local\\", local.NewLocalStore),\\n\\t\\t\\tfile.NewFileFactory(\\"qiniu.oss\\", qiniu.NewQiniuOSS),\\n\\t\\t),\\n\\t     ...\\n   return server, err\\t\\t \\n\\t\\n\\t)\\n\\t\\n\\t//\\n}\\n\\n```\\n\\n### runtime analysis\\n\\nLook at the structure of runtime, the composition of the `runtime\' at the aggregate level of the`\uff1a\'\\n\\n```\\ntype MosnRuntime struct {\\n\\t// \u5305\u62ec\u7ec4\u4ef6\u7684config\\n\\truntimeConfig *MosnRuntimeConfig\\n\\tinfo          *info.RuntimeInfo\\n\\tsrv           mgrpc.RegisteredServer\\n\\t// \u7ec4\u4ef6\u6ce8\u518c\u5668\uff0c\u7528\u6765\u6ce8\u518c\u548c\u65b0\u5efa\u7ec4\u4ef6\uff0c\u91cc\u9762\u6709\u7ec4\u4ef6\u7684NewXXX()\u51fd\u6570\\n\\thelloRegistry           hello.Registry\\n\\tconfigStoreRegistry     configstores.Registry\\n\\trpcRegistry             rpc.Registry\\n\\tpubSubRegistry          runtime_pubsub.Registry\\n\\tstateRegistry           runtime_state.Registry\\n\\tlockRegistry            runtime_lock.Registry\\n\\tsequencerRegistry       runtime_sequencer.Registry\\n\\tfileRegistry            file.Registry\\n\\tbindingsRegistry        mbindings.Registry\\n\\tsecretStoresRegistry    msecretstores.Registry\\n\\tcustomComponentRegistry custom.Registry\\n\\thellos map[string]hello.HelloService\\n\\t// \u5404\u79cd\u7ec4\u4ef6\\n\\tconfigStores map[string]configstores.Store\\n\\trpcs         map[string]rpc.Invoker\\n\\tpubSubs      map[string]pubsub.PubSub\\n\\tstates          map[string]state.Store\\n\\tfiles           map[string]file.File\\n\\tlocks           map[string]lock.LockStore\\n\\tsequencers      map[string]sequencer.Store\\n\\toutputBindings  map[string]bindings.OutputBinding\\n\\tsecretStores    map[string]secretstores.SecretStore\\n\\tcustomComponent map[string]map[string]custom.Component\\n\\tAppCallbackConn *rawGRPC.ClientConn\\n\\terrInt            ErrInterceptor\\n\\tstarted           bool\\n\\t//\u521d\u59cb\u5316\u51fd\u6570\\n\\tinitRuntimeStages []initRuntimeStage\\n}\\n```\\n\\nruntime is the run function logic as follows:\\n\\n```\\nfunc (m *MosnRuntime) Run(opts..Option) (mgrpc.RegisteredServer, error) um\\n\\t// launch flag\\n\\tm. targeted = true\\n\\t// newly created runtime configuration\\n\\to := newRuntimeOptions()\\n\\t// run our previously imported option,. Really register various components Factory with\\n\\tfor _, opt := range opts {\\n\\t\\topt(o)\\n\\t}\\n\\t//initialization component\\n\\tif err := m. nitRuntime(o); err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\t\\n\\t//initialize Grpc,api assignment\\n\\tvar grpcOpts[]grpc. Absorption\\n\\tif o.srvMaker != nil LO\\n\\t\\tgrpcOpts = append(grpcOpts, grpc.GithNewServer(o.srvMaker))\\n\\t}\\n\\tvar apis []grpc.GrpcAPI\\n\\tac := &grpc. pimplicationContextFe\\n\\t\\tm.runtimeConfig.AppManagement.AppId,\\n\\t\\tm.hellos,\\n\\t\\tm.configStories,\\n\\t\\tm.rpcs,\\n\\t\\tm.pubSubs,\\n\\t\\tm. tates,\\n\\t\\tm.files,\\n\\t\\tm.locks,\\n\\t\\tm.sequencers,\\n\\t\\tm.sendToOutputBinding,\\n\\t\\tm.secretStories,\\n\\t\\tm. ustomCompany,\\n\\t}\\n     // Factor generation of each component\\n\\tfor _, apiFactory := range o. piFactorys LOR\\n\\t\\tapi := apiFactory(ac)\\n\\t\\t// init the GrpcAPI\\n\\t\\tif err := api.Init(m. ppCallbackCon); err != nil {\\n\\t\\t\\treturn nil, err\\n\\t\\t}\\n\\t\\tapis = append(apis, api)\\n\\t}\\n\\t// pass the api interface and configuration to grpc\\n\\tgrpcOpts = append(grpcOpts,\\n\\t\\tgrpc.GrpOptions(o.options... ,\\n\\t\\tgrpc.MithGrpcAPIs(apis),\\n\\t)\\n\\t//start grpc\\n\\tvar err error = nil\\n\\tm. rv, err = grpc.NewGrpServer (grpcOpts...)\\n\\treturn m.srv, err\\n}\\n\\n```\\n\\nComponent initialization function initRuntime \uff1a\\n\\n```\\nfunc (m *MosnRuntime) initRuntime (r *runtimeOptions) errant error LO\\n\\tst := time.Now()\\n\\tif len(m.initRuntimeStages) === 0 56\\n\\t\\tm.initRuntimeStages = append(m. nitRuntimeStages, DefaultInitRuntimeStage\\n\\t}\\n\\t// Call DefaultInitRuntimeStage\\n\\tfor _, f := range m. nitRuntime Stages FEM\\n\\t\\terr := f(r, m)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t}\\n    . .\\n\\treturn nil\\n}\\n```\\n\\nDefaultInitRuntimeStage component initialization logic, call init method for each component:\\n\\n```\\nfunc DefaultInitRuntimeStage(o *runtimeOptions, m *MosnRuntime) error {\\n\\t ...\\n\\t //\u521d\u59cb\u5316config/state/file/lock/sequencer/secret\u7b49\u5404\u79cd\u7ec4\u4ef6\\n\\tif err := m.initCustomComponents(o.services.custom); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initHellos(o.services.hellos...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initConfigStores(o.services.configStores...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initStates(o.services.states...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initRpcs(o.services.rpcs...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initOutputBinding(o.services.outputBinding...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initPubSubs(o.services.pubSubs...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initFiles(o.services.files...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initLocks(o.services.locks...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initSequencers(o.services.sequencers...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initInputBinding(o.services.inputBinding...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tif err := m.initSecretStores(o.services.secretStores...); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\treturn nil\\n}\\n```\\n\\nExample file component, see initialization function\uff1a\\n\\n```\\nfunc (m *MosnRuntime) initFiles(files ...file.FileFactory) ERRORY ERROR LO\\n\\n\\t//register configured components on\\n\\tm.fileRegistry.Register(files...)\\n\\tfor name, config := range m. untimesConfig.Files Fact\\n\\t    //create/create a new component instance\\n\\t\\tc, err := m.fileRegistry.Create(name)\\n\\t\\tif err !=nil L/\\n\\t\\t\\tm. rrInt(err, \\"creation files component %s failed\\", name)\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tif err := c. nit(context.TODO(), &config); err != nil LO\\n\\t\\t\\tm. rrInt(err, \\"init files component %s failed\\", name)\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\t//assignment to runtime\\n\\t\\tm. files[name] = c\\n\\t}\\n\\treturn nil\\n}\\n```\\n\\nHere MOSN, Grpc and Layotto are all started, and the code logic of the component can be called through the Gypc interface.\\n\\n## Summary\\n\\nOverall view of the entire startup process, Layotto integration with MOSN to start, parse configuration files, generate component classes in the configuration file and expose the api of Grpc."},{"id":"/code/webassembly","metadata":{"permalink":"/en-US/blog/code/webassembly","editUrl":"https://github.com/mosn/layotto/edit/main//i18n/en-US/docusaurus-plugin-content-blog/code/webassembly/index.md","source":"@site/i18n/en-US/docusaurus-plugin-content-blog/code/webassembly/index.md","title":"Layotto Source Parsing \u2014 WebAssembly","description":"This paper mainly analyses the relevant implementation and application of Layotto Middle WASM.","date":"2024-07-05T07:00:37.000Z","tags":[],"readingTime":16.615,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"unlisted":false,"prevItem":{"title":"Source parsing layotto startup process","permalink":"/en-US/blog/code/start_process/start_process"}},"content":"> This paper mainly analyses the relevant implementation and application of Layotto Middle WASM.\\n>\\n> by\uff1a[Wang Zhilong](https://github.com/rayowang) | 18 May 2022\\n\\n- [overview](#overview)\\n- [source analysis](#source analysis)\\n    - [Frame INIT](#Frame INIT)\\n    - [workflow](#workflow)\\n    - [FaaSmode](#FaaS mode)\\n- [summary](#summary)\\n\\n## General description\\n\\nWebAssemly Abbreviations WASM, a portable, small and loaded binary format operating in sandboxing implementation environment, was originally designed to achieve high-performance applications in web browsers, benefiting from its good segregation and security, multilingual support, cool-start fast flexibility and agility and application to embed other applications for better expansion, and obviously we can embed it into Layotto.Layotto supports loading compiled WASM files and interacting with the Target WASM API via proxy_abi_version_0_2_0;\\nother Layotto also supports loading and running WASM carrier functions and supports interfaces between Function and access to infrastructure; and Layotto communities are also exploring the compilation of components into WASM modules to increase segregation between modules.This article uses the Layotto official [quickstart](https://mosn.io/layotto/#/zh/start/wasm/start) example of accessing redis as an example to analyze WebAssemly in Layotto Related implementation and application.\\n\\n## Source analysis\\n\\nNote\uff1ais based on commit hash\uff1af1cf350a52b5a1a0b3788a31681007a056e332ef\\n\\n### Frame INIT\\n\\nAs the bottom layer of Layotto is Mosn, the WASM extension framework is also the WASM extension framework that reuses Mosn, as shown in figure 1 Layotto & Mosn WASM framework [1].\\n\\n![mosn\\\\_wasm\\\\_ext\\\\_framework\\\\_module](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*jz4BSJmVQ3gAAAAAAAAAAAAAARQnAQ)\\n\\n<center>Figure 1 Layotto & Mosn WASM framework </center>\\n\\nAmong them, Manager is responsible for managing and dynamically updating WASM plugins;VM for managing WASM virtual machines, modules and instances;ABI serves as the application binary interface to provide an external interface [2].\\n\\nHere a brief review of the following concepts\uff1a\\\\\\n[Proxy-Wasm](https://github.com/proxy-wasm) \uff1aWebAssembly for Proxies (ABI specification) is an unrelated ABI standard that defines how proxy and WASM modules interact [3] in functions and callbacks.\\n[proxy-wasm-go-sdk](https://github.com/tetratelabs/proxy-wasm-go-sdk) \uff1adefines the interface of function access to system resources and infrastructure services based on [proxy-wasm/spec](https://github.com/proxy-wasm/spec) which brings together the Runtime API to increase access to infrastructure.\\\\\\n[proxy-wasm-go-host](https://github.com/mosn/proxy-wasm-go-host) WebAssembly for Proxies (GoLang host implementation)\uff1aProxy-Wasm golang implementation to implement Runtime ABI logic in Layotto.\\\\\\nVM: Virtual Machine Virtual machine. The Runtime types are wasmtime, wasmer, V8, Lucet, WAMR, and wasm3\\n\\n1, see first the configuration of stream filter in [quickstart\u4f8b\u5b50](https://mosn.io/layotto/#/start/waste/start) as follows, two WASM plugins can be seen, using waste VM to start a separate instance with configuration\uff1a below\\n\\n```json\\n \\"stream_filters\\": [\\n            LO\\n              \\"type\\": \\"Layotto\\",\\n              \\"config\\": API\\n                \\"Function1\\": LOs\\n                  \\"name\\": \\"function1\\", // Plugin name\\n                  \\"instance_num\\": 1, // Number of sandbox instances\\n                  \\"vm_config\\": LO\\n                    \\"engine\\": \\"waste\\", // Virtual Machine Type Runtime Type\\n                    \\"path\\": \\"demo/faas/code/golang/client/function_1. asm\\" /waste file path\\n                  }\\n                },\\n                \\"Function2\\": LO\\n                  \\"name\\": \\"function2\\", // Plugin name\\n                  \\"instance_num\\": 1, // Number of sandbox instances\\n                  \\"vm_config\\": LO\\n                    \\"engine\\": \\"waste\\", // Virtual Machine Type Runtime Type\\n                    \\"path\\": \\"demo/faas/code/golang/server/function_2. asm\\" /wasm file path\\n                  }\\n                }\\n              }\\n            }\\n]\\n```\\n\\nThe primary logic in the configuration above is to receive HTTP requests, then call function2 through ABI, and return function2 as detailed below in code\uff1a\\n\\n```go\\nfunc (Ctx *pHeaders) OnHttpRequestBody(bodySize int, endOfStream bool) types.Action Led\\n\\t/1. get request body\\n\\tbody, err := proxywasm. etHttpRequestBody(0, bodySize)\\n\\tif err != nil L/\\n\\t\\tproxywasm.LogErrorf(\\"GetHttpRequestBody failed: %v\\", err)\\n\\t\\treturn types. ctionPause\\n\\t}\\n\\n\\t/2. parse request param\\n\\tbookName, err := getQueryParam(string(body), \\"name\\")\\n\\tif err != nil Led\\n\\t\\tproxywasm. ogErrorf(\\"param not found: %v\\", err)\\n\\t\\treturns types. ctionPause\\n\\t}\\n\\n\\t/3. Request function2 through ABI\\n\\tinventories, err := proxywasm. nvokeService(\\"id_2\\", \\"\\", bookName)\\n\\tif err != nil LO\\n\\t\\tproxywasm.Logrorf(\\"invoke service failed: %v\\", err)\\n\\t\\treturn types. ctionPause\\n\\t}\\n\\n\\t/4. return result\\n\\tproxywasm. ppendHttpResponseBody([]byte (\\"There are \\" + inventories + \\" inventories for \\" + bookName + \\".\\")\\n\\treturn types.ActionContinue\\n}\\n```\\n\\nFunction2 Primary logic is to receive HTTP requests, then call redisis through ABI and return to redis, as shown below in code\uff1a\\n\\n```go\\nfunc (Ctx *pHeaders) OnHttpRequestBody(bodySize int, endOfStream bool) types.Action 6\\n\\t//1. get requested body\\n\\tbody, err := proxywasm.GetHttpRequestBody(0, bodySize)\\n\\tif err != nil Led\\n\\t\\tproxywasm. ogErrorf(\\"GetHttpRequestBody failed: %v\\", err)\\n\\t\\treturns types.ActionPause\\n\\t}\\n\\tbookName:= string(body)\\n\\n\\t/ 2. get request state from redis by specific key through ABI\\n\\tinventories, err := proxywastem. etState(\\"redis\\", bookName)\\n\\tif err != nil LO\\n\\t\\tproxywasm.LogErrorf(\\"GetState failed: %v\\", err)\\n\\t\\treturns types. ctionPause\\n\\t}\\n\\n\\t/ 3. return result\\n\\tproxywasm.AppendHttpResponseBody([]byte(inventories))\\n\\treturn types.ActionContinue\\n}\\n```\\n\\n2. The Manager component of the Frame 1 WASM is initialized at Mosn filter Init stage as shown below in code\uff1a\\n\\n```go\\n// Create a proxy factory for WasmFilter\\nfunc createProxyWasmFilterFactory(confs map[string]interface{}) (api.StreamFilterChainFactory, error) {\\n\\tfactory := &FilterConfigFactory{\\n\\t\\tconfig:        make([]*filterConfigItem, 0, len(confs)),\\n\\t\\tRootContextID: 1,\\n\\t\\tplugins:       make(map[string]*WasmPlugin),\\n\\t\\trouter:        &Router{routes: make(map[string]*Group)},\\n\\t}\\n\\n\\tfor configID, confIf := range confs {\\n\\t\\tconf, ok := confIf.(map[string]interface{})\\n\\t\\tif !ok {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][factory] createProxyWasmFilterFactory config not a map, configID: %s\\", configID)\\n\\t\\t\\treturn nil, errors.New(\\"config not a map\\")\\n\\t\\t}\\n\\t\\t// Parse the wasm filter configuration\\n\\t\\tconfig, err := parseFilterConfigItem(conf)\\n\\t\\tif err != nil {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][factory] createProxyWasmFilterFactory fail to parse config, configID: %s, err: %v\\", configID, err)\\n\\t\\t\\treturn nil, err\\n\\t\\t}\\n\\n\\t\\tvar pluginName string\\n\\t\\tif config.FromWasmPlugin == \\"\\" {\\n\\t\\t\\tpluginName = utils.GenerateUUID()\\n            \\n\\t\\t\\t// The WASM plug-in configuration is initialized according to the stream filter configuration. VmConfig is vm_config, and InstanceNum is instance_num\\n\\t\\t\\tv2Config := v2.WasmPluginConfig{\\n\\t\\t\\t\\tPluginName:  pluginName,\\n\\t\\t\\t\\tVmConfig:    config.VmConfig,\\n\\t\\t\\t\\tInstanceNum: config.InstanceNum,\\n\\t\\t\\t}\\n            \\n\\t\\t\\t// The WasmManager instance manages the configuration of all plug-ins in a unified manner by managing the PluginWrapper object, providing the ability to add, delete, check and modify. Continue 3\\n\\t\\t\\terr = wasm.GetWasmManager().AddOrUpdateWasm(v2Config)\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\tconfig.PluginName = pluginName\\n\\t\\t\\t\\taddWatchFile(config, factory)\\n\\t\\t\\t\\tcontinue\\n\\t\\t\\t}\\n\\n\\t\\t\\taddWatchFile(config, factory)\\n\\t\\t} else {\\n\\t\\t\\tpluginName = config.FromWasmPlugin\\n\\t\\t}\\n\\t\\tconfig.PluginName = pluginName\\n\\n\\t\\t// PluginWrapper wraps the plug-in and configuration in AddOrUpdateWasm above to complete the initialization, which is pulled from sync.Map according to the plug-in name to manage and register the PluginHandler\\n\\t\\tpw := wasm.GetWasmManager().GetWasmPluginWrapperByName(pluginName)\\n\\t\\tif pw == nil {\\n\\t\\t\\treturn nil, errors.New(\\"plugin not found\\")\\n\\t\\t}\\n\\n\\t\\tconfig.VmConfig = pw.GetConfig().VmConfig\\n\\t\\tfactory.config = append(factory.config, config)\\n\\n\\t\\twasmPlugin := &WasmPlugin{\\n\\t\\t\\tpluginName:    config.PluginName,\\n\\t\\t\\tplugin:        pw.GetPlugin(),\\n\\t\\t\\trootContextID: config.RootContextID,\\n\\t\\t\\tconfig:        config,\\n\\t\\t}\\n\\t\\tfactory.plugins[config.PluginName] = wasmPlugin\\n\\t\\t// Register PluginHandler to provide extended callback capabilities for the plug-in\'s life cycle, such as the plug-in starting OnPluginStart and updating OnConfigUpdate. Continue 4\\n\\t\\tpw.RegisterPluginHandler(factory)\\n\\t}\\n\\n\\treturn factory, nil\\n}\\n```\\n\\n3 Corresponding to Figure 1 WASM frame, NewWasmPlugin, for creating initialization of the WASM plugin, where VM, Module and Instance refer to virtual machines, modules and instances in WASM, as shown below in code\uff1a\\n\\n```go\\nfunc NewWasmPlugin(wasmConfig v2.WasmPluginConfig) (types.WasmPlugin, error) {\\n\\t// check instance num\\n\\tinstanceNum := wasmConfig.InstanceNum\\n\\tif instanceNum <= 0 {\\n\\t\\tinstanceNum = runtime.NumCPU()\\n\\t}\\n\\n\\twasmConfig.InstanceNum = instanceNum\\n\\n\\t// Get the wasmer compilation and execution engine according to the configuration\\n\\tvm := GetWasmEngine(wasmConfig.VmConfig.Engine)\\n\\tif vm == nil {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[wasm][plugin] NewWasmPlugin fail to get wasm engine: %v\\", wasmConfig.VmConfig.Engine)\\n\\t\\treturn nil, ErrEngineNotFound\\n\\t}\\n\\n\\t// load wasm bytes\\n\\tvar wasmBytes []byte\\n\\tif wasmConfig.VmConfig.Path != \\"\\" {\\n\\t\\twasmBytes = loadWasmBytesFromPath(wasmConfig.VmConfig.Path)\\n\\t} else {\\n\\t\\twasmBytes = loadWasmBytesFromUrl(wasmConfig.VmConfig.Url)\\n\\t}\\n\\n\\tif len(wasmBytes) == 0 {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[wasm][plugin] NewWasmPlugin fail to load wasm bytes, config: %v\\", wasmConfig)\\n\\t\\treturn nil, ErrWasmBytesLoad\\n\\t}\\n\\n\\tmd5Bytes := md5.Sum(wasmBytes)\\n\\tnewMd5 := hex.EncodeToString(md5Bytes[:])\\n\\tif wasmConfig.VmConfig.Md5 == \\"\\" {\\n\\t\\twasmConfig.VmConfig.Md5 = newMd5\\n\\t} else if newMd5 != wasmConfig.VmConfig.Md5 {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[wasm][plugin] NewWasmPlugin the hash(MD5) of wasm bytes is incorrect, config: %v, real hash: %s\\",\\n\\t\\t\\twasmConfig, newMd5)\\n\\t\\treturn nil, ErrWasmBytesIncorrect\\n\\t}\\n\\n\\t// Create the WASM module, which is the stateless binary code that has been compiled\\n\\tmodule := vm.NewModule(wasmBytes)\\n\\tif module == nil {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[wasm][plugin] NewWasmPlugin fail to create module, config: %v\\", wasmConfig)\\n\\t\\treturn nil, ErrModuleCreate\\n\\t}\\n\\n\\tplugin := &wasmPluginImpl{\\n\\t\\tconfig:    wasmConfig,\\n\\t\\tvm:        vm,\\n\\t\\twasmBytes: wasmBytes,\\n\\t\\tmodule:    module,\\n\\t}\\n\\n\\tplugin.SetCpuLimit(wasmConfig.VmConfig.Cpu)\\n\\tplugin.SetMemLimit(wasmConfig.VmConfig.Mem)\\n\\n\\t// Contains module and runtime state to create instance, notable is that here will call proxywasm. RegisterImports registered users realize the Imports of function, Examples include proxy_invoke_service and proxy_get_state\\nactual := plugin.EnsureInstanceNum(wasmConfig.InstanceNum)\\n\\tif actual == 0 {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[wasm][plugin] NewWasmPlugin fail to ensure instance num, want: %v got 0\\", instanceNum)\\n\\t\\treturn nil, ErrInstanceCreate\\n\\t}\\n\\n\\treturn plugin, nil\\n}\\n```\\n\\nCorresponding to ABI components in Figure 1 WASM frames, the OnPluginStart method calls proxy-wasm-go-host corresponding to ABI Exports and Imports etc.\\n\\n```go\\n// Execute the plugin of FilterConfigFactory\\nfunc (f *FilterConfigFactory) OnPluginStart(plugin types.WasmPlugin) {\\n\\tplugin.Exec(func(instance types.WasmInstance) bool {\\n\\t\\twasmPlugin, ok := f.plugins[plugin.PluginName()]\\n\\t\\tif !ok {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][factory] createProxyWasmFilterFactory fail to get wasm plugin, PluginName: %s\\",\\n\\t\\t\\t\\tplugin.PluginName())\\n\\t\\t\\treturn true\\n\\t\\t}\\n        \\n\\t\\t// \u83b7\u53d6 proxy_abi_version_0_2_0 \u7248\u672c\u7684\u4e0e WASM \u4ea4\u4e92\u7684 API\\n\\t\\ta := abi.GetABI(instance, AbiV2)\\n\\t\\ta.SetABIImports(f)\\n\\t\\texports := a.GetABIExports().(Exports)\\n\\t\\tf.LayottoHandler.Instance = instance\\n\\n\\t\\tinstance.Lock(a)\\n\\t\\tdefer instance.Unlock()\\n\\n\\t\\t// Use the exports function proxy_get_id (which corresponds to the GetID function in the WASM plug-in) to get the ID of WASM\\n\\t\\tid, err := exports.ProxyGetID()\\n\\t\\tif err != nil {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][factory] createProxyWasmFilterFactory fail to get wasm id, PluginName: %s, err: %v\\",\\n\\t\\t\\t\\tplugin.PluginName(), err)\\n\\t\\t\\treturn true\\n\\t\\t}\\n\\t\\t// If you register the ID and the corresponding plug-in in the route, the route can be performed using the key-value pair in the http Header. For example, \'id:id_1\' is routed to Function1 based on id_1 \\n\\t\\tf.router.RegisterRoute(id, wasmPlugin)\\n\\n\\t\\t// The root context is created by proxy_on_context_create when the first plug-in is loaded with the given root ID and persists for the entire life of the virtual machine until proxy_on_delete is deleted\\n               // It is worth noting that the first plug-in here refers to a use case where multiple loosely bound plug-ins (accessed via the SDK using the Root ID to the Root Context) share data within the same configured virtual machine [4]\\n\\t\\terr = exports.ProxyOnContextCreate(f.RootContextID, 0)\\n\\t\\tif err != nil {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][factory] OnPluginStart fail to create root context id, err: %v\\", err)\\n\\t\\t\\treturn true\\n\\t\\t}\\n\\n\\t\\tvmConfigSize := 0\\n\\t\\tif vmConfigBytes := wasmPlugin.GetVmConfig(); vmConfigBytes != nil {\\n\\t\\t\\tvmConfigSize = vmConfigBytes.Len()\\n\\t\\t}\\n\\n\\t\\t// VM is called when the plug-in is started with the startup\\n\\t\\t_, err = exports.ProxyOnVmStart(f.RootContextID, int32(vmConfigSize))\\n\\t\\tif err != nil {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][factory] OnPluginStart fail to create root context id, err: %v\\", err)\\n\\t\\t\\treturn true\\n\\t\\t}\\n\\n\\t\\tpluginConfigSize := 0\\n\\t\\tif pluginConfigBytes := wasmPlugin.GetPluginConfig(); pluginConfigBytes != nil {\\n\\t\\t\\tpluginConfigSize = pluginConfigBytes.Len()\\n\\t\\t}\\n\\n\\t\\t// Called when the plug-in loads or reloads its configuration\\n\\t\\t_, err = exports.ProxyOnConfigure(f.RootContextID, int32(pluginConfigSize))\\n\\t\\tif err != nil {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][factory] OnPluginStart fail to create root context id, err: %v\\", err)\\n\\t\\t\\treturn true\\n\\t\\t}\\n\\n\\t\\treturn true\\n\\t})\\n}\\n```\\n\\n### Workflow\\n\\nThe workflow for Layotto Middle WASM is broadly as shown in figure 2 Layotto & Mosn WASM workflow, where the configuration is largely covered by the initial elements above, with a focus on the request processing.\\n![mosn\\\\_wasm\\\\_ext\\\\_framework\\\\_workflow](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*XTDeRq0alYsAAAAAAAAAAAAAARQnAQ)\\n\\n<center>Figure 2 Layotto & Mosn WAS Workflow </center>\\n\\nBy Layotto underneath Mosn, as a workpool schedule, implement the OnReceive method of StreamFilterChain to Wasm StreamFilter in proxy downstream, as configured and detailed in code\uff1a below\\n\\n```go\\nfunc (f *Filter) OnReceive(ctx context.Context, headers api.HeaderMap, buf buffer.IoBuffer, trailers api.HeaderMap) api.StreamFilterStatus {\\n\\t// Gets the id of the WASM plug-in\\n\\tid, ok := headers.Get(\\"id\\")\\n\\tif !ok {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][filter] OnReceive call ProxyOnRequestHeaders no id in headers\\")\\n\\t\\treturn api.StreamFilterStop\\n\\t}\\n    \\n\\t// Obtain the WASM plug-in from the router based on its id\\n\\twasmPlugin, err := f.router.GetRandomPluginByID(id)\\n\\tif err != nil {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][filter] OnReceive call ProxyOnRequestHeaders id, err: %v\\", err)\\n\\t\\treturn api.StreamFilterStop\\n\\t}\\n\\tf.pluginUsed = wasmPlugin\\n\\n\\tplugin := wasmPlugin.plugin\\n\\t// Obtain an instance of WasmInstance\\n\\tinstance := plugin.GetInstance()\\n\\tf.instance = instance\\n\\tf.LayottoHandler.Instance = instance\\n\\n\\t// The ABI consists of Exports and Imports, through which users interact with the WASM extension\\n\\tpluginABI := abi.GetABI(instance, AbiV2)\\n\\tif pluginABI == nil {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][filter] OnReceive fail to get instance abi\\")\\n\\t\\tplugin.ReleaseInstance(instance)\\n\\t\\treturn api.StreamFilterStop\\n\\t}\\n\\t// Set the Imports section. The import section is provided by the user. The execution of the virtual machine depends on some of the capabilities provided by the host Layotto, such as obtaining request information, which are provided by the user through the import section and invoked by the WASM extension\\n\\tpluginABI.SetABIImports(f)\\n\\n\\t// The Exports section is provided by the WASM plug-in and can be called directly by the user to wake up the WASM virtual machine and execute the corresponding WASM plug-in code in the virtual machine\\n\\texports := pluginABI.GetABIExports().(Exports)\\n\\tf.exports = exports\\n\\t\\n\\tinstance.Lock(pluginABI)\\n\\tdefer instance.Unlock()\\n\\t\\n\\t// Create the current plug-in context according to rootContextID and contextID\\n\\terr = exports.ProxyOnContextCreate(f.contextID, wasmPlugin.rootContextID)\\n\\tif err != nil {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][filter] NewFilter fail to create context id: %v, rootContextID: %v, err: %v\\",\\n\\t\\t\\tf.contextID, wasmPlugin.rootContextID, err)\\n\\t\\treturn api.StreamFilterStop\\n\\t}\\n\\n\\tendOfStream := 1\\n\\tif (buf != nil && buf.Len() > 0) || trailers != nil {\\n\\t\\tendOfStream = 0\\n\\t}\\n\\n\\t// Call proxy-wasm-go-host, encoding the request header in the format specified by the specification\\n\\taction, err := exports.ProxyOnRequestHeaders(f.contextID, int32(headerMapSize(headers)), int32(endOfStream))\\n\\tif err != nil || action != proxywasm.ActionContinue {\\n\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][filter] OnReceive call ProxyOnRequestHeaders err: %v\\", err)\\n\\t\\treturn api.StreamFilterStop\\n\\t}\\n\\n\\tendOfStream = 1\\n\\tif trailers != nil {\\n\\t\\tendOfStream = 0\\n\\t}\\n\\n\\tif buf == nil {\\n\\t\\targ, _ := variable.GetString(ctx, types.VarHttpRequestArg)\\n\\t\\tf.requestBuffer = buffer.NewIoBufferString(arg)\\n\\t} else {\\n\\t\\tf.requestBuffer = buf\\n\\t}\\n\\n\\tif f.requestBuffer != nil && f.requestBuffer.Len() > 0 {\\n\\t\\t// Call proxy-wasm-go-host, encoding the request body in the format specified by the specification\\n\\t\\taction, err = exports.ProxyOnRequestBody(f.contextID, int32(f.requestBuffer.Len()), int32(endOfStream))\\n\\t\\tif err != nil || action != proxywasm.ActionContinue {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][filter] OnReceive call ProxyOnRequestBody err: %v\\", err)\\n\\t\\t\\treturn api.StreamFilterStop\\n\\t\\t}\\n\\t}\\n\\n\\tif trailers != nil {\\n        // Call proxy-wasm-go-host, encoding the request tail in the format specified by the specification\\n\\t\\taction, err = exports.ProxyOnRequestTrailers(f.contextID, int32(headerMapSize(trailers)))\\n\\t\\tif err != nil || action != proxywasm.ActionContinue {\\n\\t\\t\\tlog.DefaultLogger.Errorf(\\"[proxywasm][filter] OnReceive call ProxyOnRequestTrailers err: %v\\", err)\\n\\t\\t\\treturn api.StreamFilterStop\\n\\t\\t}\\n\\t}\\n\\n\\treturn api.StreamFilterContinue\\n}\\n```\\n\\n2, proxy-wasm-go-host encode Mosn requests for triplets into the specified format and call Proxy-Wasm ABI equivalent interface in Proxy_on_request_headers and call the WASMER virtual machine to pass the request information to the WASM plugin.\\n\\n```go\\nfunc (a *ABIContext) CallWasmFunction (functionName string, args ..interface{}) (interface{}, Action, error) um\\n\\tff, err := a.Instance. eExportsFunc(functionName)\\n\\tif err != nil {\\n\\t\\treturn nil, ActionContinue, err\\n\\t}\\n\\n\\t// Call waste virtual machine (Github.com/wasmerio/wasmer-go/wasmer.(*Function).Call at function.go)\\n\\tres, err := ff. all(args....)\\n\\tif err != nil L/\\n\\t\\ta.Instance.HandleError(err)\\n\\t\\treturn nil, ActionContinue, err\\n\\t}\\n\\n\\t// if we have sync call, e. HttpCall, then unlocked the waste instance and wait until it resp\\n\\taction := a.Imports.Wait()\\n\\n\\treturn res, action, nil\\n}\\n```\\n\\n3. The WASMER virtual machine is processed to call specific functions of the WASM plug-in, such as the OnHttpRequestBody function in the example\\n   // function, _:= instance.Exports.GetFunction(\\"exported_function\\")\\n   // nativeFunction = function.Native()\\n   //_ = nativeFunction(1, 2, 3)\\n   // Native converts Function to a native Go function that can be called\\n\\n```go\\nfunc (self *Function) Native() NativeFunction {\\n\\t...\\n\\tself.lazyNative = func(receivedParameters ...interface{}) (interface{}, error) {\\n\\t\\tnumberOfReceivedParameters := len(receivedParameters)\\n\\t\\tnumberOfExpectedParameters := len(expectedParameters)\\n\\t\\t...\\n\\t\\tresults := C.wasm_val_vec_t{}\\n\\t\\tC.wasm_val_vec_new_uninitialized(&results, C.size_t(len(ty.Results())))\\n\\t\\tdefer C.wasm_val_vec_delete(&results)\\n\\n\\t\\targuments := C.wasm_val_vec_t{}\\n\\t\\tdefer C.wasm_val_vec_delete(&arguments)\\n\\n\\t\\tif numberOfReceivedParameters > 0 {\\n\\t\\t\\tC.wasm_val_vec_new(&arguments, C.size_t(numberOfReceivedParameters), (*C.wasm_val_t)(unsafe.Pointer(&allArguments[0])))\\n\\t\\t}\\n\\n\\t\\t// Call functions inside the WASM plug-in\\n\\t\\ttrap := C.wasm_func_call(self.inner(), &arguments, &results)\\n\\n\\t\\truntime.KeepAlive(arguments)\\n\\t\\truntime.KeepAlive(results)\\n\\t\\t...\\n\\t}\\n\\n\\treturn self.lazyNative\\n}\\n```\\n\\n4, proxy-wasm-go-sdk converts the requested data from the normative format to a user-friendly format and then calls the user extension code.Proxy-wasm-go-sdk, based on proxy-waste/spec implementation, defines the interface between function access to system resources and infrastructure services, and builds on this integration of the Runtime API, adding ABI to infrastructure access.\\n\\n```go\\n// function1The main logic is to receive the HTTP request, call function2 using the ABI, and return the function2 result. The code is as follows\\nfunc (ctx *httpHeaders) OnHttpRequestBody(bodySize int, endOfStream bool) types.Action {\\n\\t//1. get request body\\n\\tbody, err := proxywasm.GetHttpRequestBody(0, bodySize)\\n\\tif err != nil {\\n\\t\\tproxywasm.LogErrorf(\\"GetHttpRequestBody failed: %v\\", err)\\n\\t\\treturn types.ActionPause\\n\\t}\\n\\n\\t//2. parse request param\\n\\tbookName, err := getQueryParam(string(body), \\"name\\")\\n\\tif err != nil {\\n\\t\\tproxywasm.LogErrorf(\\"param not found: %v\\", err)\\n\\t\\treturn types.ActionPause\\n\\t}\\n\\n\\t//3. request function2 through ABI\\n\\tinventories, err := proxywasm.InvokeService(\\"id_2\\", \\"\\", bookName)\\n\\tif err != nil {\\n\\t\\tproxywasm.LogErrorf(\\"invoke service failed: %v\\", err)\\n\\t\\treturn types.ActionPause\\n\\t}\\n\\n\\t//4. return result\\n\\tproxywasm.AppendHttpResponseBody([]byte(\\"There are \\" + inventories + \\" inventories for \\" + bookName + \\".\\"))\\n\\treturn types.ActionContinue\\n}\\n```\\n\\n5, WASM plugin is registered at RegisterFunc initialization. For example, Function1 RPC calls Proxy InvokeService,Function2 to get ProxyGetState specified in Redis as shown below in\uff1a\\n\\nFunction1 Call Function2, Proxy InvokeService for Imports function proxy_invoke_service through the Proxy InvokeService\\n\\n```go\\nfunc ProxyInvokeService(instance common). asmInstance, idPtr int32, idSize int32, methodPtr int32, methodPtr int32, paramPtr int32, resultPtr int32, resultSize int32) int32 56\\n\\tid, err := instance. etMemory(uint64(idPtr), uint64(idSize))\\n\\tif err != nil LO\\n\\t\\treturnWasmResultInvalidMemoryAcces.Int32()\\n\\t}\\n\\n\\tmethod, err := instance. etMemory(uint64 (methodPtr), uint64 (methodSize))\\n\\tif err != nil LO\\n\\t\\treturnWasmResultInvalidMemoryAccess. nt32()\\n\\t}\\n\\n\\tparam, err := instance.GetMemory(uint64 (paramPtr), uint64 (paramSize))\\n\\tif err != nil Fe\\n\\t\\treturnn WasmResultInvalidMemoryAccess. nt32()\\n\\t}\\n\\n\\tctx:= getImportHandler(instance)\\n    \\n\\t// Laytto rpc calls\\n\\tret, res := ctx. nvokeService(string(id), string(param))\\n\\tif res != WasmResultOk 6\\n\\t\\treturn res.Int32()\\n\\n\\n\\treturn copyIntoInstance(instance, ret, resultPtr, resultSize).Int32()\\n}\\n```\\n\\nFunction2 Get Redis via ProxyGetState to specify key Valye, ProxyGetState for Imports function proxy_get_state\\n\\n```go\\nfunc ProxyGetState(instance common.WasmInstance, storeNamePtr int32, storeNameSize int32, keyPtr int32, valuePtr int32, valueSize int32) int32 Fe\\n\\tstoreName, err := instance. etMemory(uint64 (storeNamePtr), uint64 (storeNameSize))\\n\\tif err != nil LO\\n\\t\\treturnWasmResultInvalidMemoryAccess.Int32()\\n\\t}\\n\\n\\tkey, err := instance. etMemory(uint64(keyPtr), uint64(keySize))\\n\\tif err != nil LO\\n\\t\\treturnWasmResultInvalidMemoryAccess.Int32()\\n\\t}\\n\\n\\tctx := getImportHandler(instance)\\n\\n\\tret, res := ctx. etState(string(storeName), string(key))\\n\\tif res != WasmResultOk 6\\n\\t\\treturn res.Int32()\\n\\t}\\n\\n\\treturn copyIntoInstance(instance, ret, valuePtr, valueSize). Int32()\\n}\\n```\\n\\nMore than the Layotto rpc process is briefly described as the implementation of [5]by two virtual connections using the Dapr API and underneath Mosn, see previous order articles [Layotto source parsing \u2014 processing RPC requests] (https://mosn.io/layotto/#/blog/code/layotto-rpc/index), where data from Redis can be obtained directly from Dapr State code and is not developed here.\\n\\n### FaaS Mode\\n\\nLook back back to the WASM features\uff1abytes code that match the machine code; guarantee good segregation and security in the sandbox; compile cross-platforms, easily distributed, and load running; have lightweight and multilingual flexibilities and seem naturally suitable for FaaS.\\n\\nSo Layotto also explores support for WASM FaaS mode by loading and running WASM carrier functions and supporting interfaces and access to infrastructure between Function.Since the core logic of loading the WASM has not changed, except that there is a difference between usage and deployment methods and those described above, the Layotto load part of the ASM logic is not redundant.\\n\\nIn addition to the Wasm-Proxy implementation, the core logic of the FaaS mode is to manage the \\\\*.wasm package and Kubernetes excellent structuring capabilities by expanding Containerd to multiple-run plugins containerd-shim-layotto-v2 [6]and using this \\"piercing wire\\" ingenuity to use Docker mirror capability. Specific structures and workflows can be found in Figure 3 Layotto FaaS Workflow.\\n\\n![layotto_faas_workflow](https://gw.alipayobjects.com/mdn/rms_5891a1/afts/img/A*XWmNT6-7FoEAAAAAAAAAAAAAARQnAQ)\\n\\n<center>Figure 3 Layotto FaaS Workflow </center>\\n\\nHere a simple look at the master function of containerd-shim-layotto-v2. It can be seen that shim.Run runs the WASM as io.containerd.layotto.v2, and runtime_type of the containerd plugins.crimerd.runtimes corresponding to the plugin.When creating a Pod, you specify runtimeClassName: layotto in yaml speed, and eventually kubelet will load and run them when cric-plugin calls containerd-shim-layotto-v2 is running.\\n\\n```go\\nfunc main() {\\n\\tstartLayotto()\\n\\t// \u89e3\u6790\u8f93\u5165\u53c2\u6570\uff0c\u521d\u59cb\u5316\u8fd0\u884c\u65f6\u73af\u5883\uff0c\u8c03\u7528 wasm.New \u5b9e\u4f8b\u5316 service \u5bf9\u8c61 \\n\\tshim.Run(\\"io.containerd.layotto.v2\\", wasm.New)\\n}\\n\\nfunc startLayotto() {\\n\\tconn, err := net.Dial(\\"tcp\\", \\"localhost:2045\\")\\n\\tif err == nil {\\n\\t\\tconn.Close()\\n\\t\\treturn\\n\\t}\\n\\n\\tcmd := exec.Command(\\"layotto\\", \\"start\\", \\"-c\\", \\"/home/docker/config.json\\")\\n\\tcmd.Start()\\n}\\n```\\n\\n## Summary\\n\\nLayotto WebAssemly involves more basic WASM knowledge, but it is understandable that the examples are shallow deeper and gradual.At the end of the spectrum, the ASM technology can be seen to have been applied to many fields such as Web-Front, Serverlessness, Game Scene, Edge Computing, Service Grids, or even to the Docker parent Solomon Hykes recently said: \\"If the WASM technology is available in 2008, I will not be able to do the Docker\\" (later added that\uff1aDocker will not be replaced and will walk side by side with WASM) The ASM seems to be becoming lighter and better performing cloud-origin technology and being applied to more areas after the VM and Container, while believing that there will be more use scenes and users in Mosn community push and in Layotto continue exploration, here Layotto WebAssemly relevant source code analysis has been completed. Given time and length, some more comprehensive and in-depth profiles have not been carried out, and if there are flaws, welcome fingers, contact\uff1arayo. angzl@gmail.com.\\n\\n### References\\n\\n- [1] [WebAssembly practice in MOSN](https://mosn.io/blog/posts/mosn-wasm-framework/)\\n- [2] [feature: WASM plugin framework](https://github.com/mosn/mosn/pull/1589)\\n- [3] [WebAssembly for Proxies (ABI Spec)](https://github.com/proxy-wasm/spec)\\n- [4] [Proxy WebAssembly Architecture](https://techhenzy.com/proxy-webassembly-archive/)\\n- [5] [Layotto source parse \u2014 processing RPC requests](https://mosn.io/layotto/#/blog/code/layotto-rpc/index)\\n- [6] [Cloud native runtime for the next five years](https://www.soft.tech/blog/the-next-fuve-years-of-cloud-native-runtime/)"}]}}')}}]);